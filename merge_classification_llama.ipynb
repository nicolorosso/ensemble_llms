{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\nrosso\\Documents\\thesis_project\\notebooks\\Active_Learning\\llama_cleaned_final_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>final_classification</th>\n",
       "      <th>confidence</th>\n",
       "      <th>individual_results</th>\n",
       "      <th>sparse_embedding</th>\n",
       "      <th>dense_embedding</th>\n",
       "      <th>cleaned_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>God-Man vs. Science-Hero: http://www.gocomics....</td>\n",
       "      <td>godman vs sciencehero</td>\n",
       "      <td>proscience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'llama3.1:8b': 'proscience'}</td>\n",
       "      <td>SparseEmbedding(indices=[1998, 2080, 2114, 238...</td>\n",
       "      <td>[-0.019969597458839417, 0.06492504477500916, -...</td>\n",
       "      <td>proscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>#Sciencemob #Proscience The Mass Libel Reform ...</td>\n",
       "      <td>the mass libel reform blog fight for free spee...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'llama3.1:8b': 'neutral'}</td>\n",
       "      <td>SparseEmbedding(indices=[2114, 2375, 2489, 249...</td>\n",
       "      <td>[-0.02676120400428772, -0.005197218619287014, ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>What is wrong w people?! #proscience RT @NewHu...</td>\n",
       "      <td>what is wrong w people rt and just like that t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'llama3.1:8b': 'neutral'}</td>\n",
       "      <td>SparseEmbedding(indices=[1059, 2025, 2054, 205...</td>\n",
       "      <td>[0.005084160715341568, -0.016203228384256363, ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>I’m too pro-science to be pro-choice http://ow...</td>\n",
       "      <td>im too proscience to be prochoice</td>\n",
       "      <td>proscience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'llama3.1:8b': 'proscience'}</td>\n",
       "      <td>SparseEmbedding(indices=[2017, 2022, 2025, 210...</td>\n",
       "      <td>[-0.00469502666965127, 0.027932850643992424, -...</td>\n",
       "      <td>proscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Video: Glenn Beck - MSNBC Anti-God Network htt...</td>\n",
       "      <td>video glenn beck msnbc antigod network antigod...</td>\n",
       "      <td>antiscience</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'llama3.1:8b': 'antiscience'}</td>\n",
       "      <td>SparseEmbedding(indices=[1998, 2040, 2143, 227...</td>\n",
       "      <td>[-0.05969066917896271, 0.009564031846821308, -...</td>\n",
       "      <td>antiscience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                              tweet  \\\n",
       "0           0      3  God-Man vs. Science-Hero: http://www.gocomics....   \n",
       "1           1      4  #Sciencemob #Proscience The Mass Libel Reform ...   \n",
       "2           2      5  What is wrong w people?! #proscience RT @NewHu...   \n",
       "3           3      6  I’m too pro-science to be pro-choice http://ow...   \n",
       "4           4      7  Video: Glenn Beck - MSNBC Anti-God Network htt...   \n",
       "\n",
       "                                        preprocessed final_classification  \\\n",
       "0                              godman vs sciencehero           proscience   \n",
       "1  the mass libel reform blog fight for free spee...              neutral   \n",
       "2  what is wrong w people rt and just like that t...              neutral   \n",
       "3                  im too proscience to be prochoice           proscience   \n",
       "4  video glenn beck msnbc antigod network antigod...          antiscience   \n",
       "\n",
       "   confidence              individual_results  \\\n",
       "0         1.0   {'llama3.1:8b': 'proscience'}   \n",
       "1         1.0      {'llama3.1:8b': 'neutral'}   \n",
       "2         1.0      {'llama3.1:8b': 'neutral'}   \n",
       "3         1.0   {'llama3.1:8b': 'proscience'}   \n",
       "4         1.0  {'llama3.1:8b': 'antiscience'}   \n",
       "\n",
       "                                    sparse_embedding  \\\n",
       "0  SparseEmbedding(indices=[1998, 2080, 2114, 238...   \n",
       "1  SparseEmbedding(indices=[2114, 2375, 2489, 249...   \n",
       "2  SparseEmbedding(indices=[1059, 2025, 2054, 205...   \n",
       "3  SparseEmbedding(indices=[2017, 2022, 2025, 210...   \n",
       "4  SparseEmbedding(indices=[1998, 2040, 2143, 227...   \n",
       "\n",
       "                                     dense_embedding cleaned_classification  \n",
       "0  [-0.019969597458839417, 0.06492504477500916, -...             proscience  \n",
       "1  [-0.02676120400428772, -0.005197218619287014, ...                neutral  \n",
       "2  [0.005084160715341568, -0.016203228384256363, ...                neutral  \n",
       "3  [-0.00469502666965127, 0.027932850643992424, -...             proscience  \n",
       "4  [-0.05969066917896271, 0.009564031846821308, -...            antiscience  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['sparse_embedding', 'dense_embedding'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('llama_no_embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 80/80 [09:09<00:00,  6.87s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: (79763, 9)\n",
      "Sample embedding shape: (384,)\n",
      "Embedding dimension verified: 384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class SentenceTransformerEmbedder:\n",
    "    def __init__(self, model: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model)\n",
    "\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        return self.model.encode(texts, show_progress_bar=False)\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame, embedder: SentenceTransformerEmbedder) -> pd.DataFrame:\n",
    "    chunk['dense_embedding'] = list(embedder.embed(chunk['preprocessed'].tolist()))\n",
    "    return chunk\n",
    "\n",
    "def save_progress(chunk: pd.DataFrame, chunk_id: int, output_dir: str):\n",
    "    output_path = os.path.join(output_dir, f\"chunk_{chunk_id}.pkl\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n",
    "\n",
    "def load_progress(output_dir: str) -> List[pd.DataFrame]:\n",
    "    chunks = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.startswith(\"chunk_\") and filename.endswith(\".pkl\"):\n",
    "            with open(os.path.join(output_dir, filename), 'rb') as f:\n",
    "                chunks.append(pickle.load(f))\n",
    "    return chunks\n",
    "\n",
    "def create_embedding_pipeline(df: pd.DataFrame, chunk_size: int, output_dir: str) -> pd.DataFrame:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    embedder = SentenceTransformerEmbedder('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Check for existing progress\n",
    "    existing_chunks = load_progress(output_dir)\n",
    "    if existing_chunks:\n",
    "        print(f\"Found {len(existing_chunks)} existing chunks. Resuming from the last processed chunk.\")\n",
    "        df = pd.concat([chunk for chunk in existing_chunks if 'dense_embedding' in chunk.columns])\n",
    "        start_index = len(df)\n",
    "    else:\n",
    "        start_index = 0\n",
    "\n",
    "    # Process remaining data in chunks\n",
    "    for i in tqdm(range(start_index, len(df), chunk_size), desc=\"Processing chunks\"):\n",
    "        chunk = df.iloc[i:i+chunk_size].copy()\n",
    "        processed_chunk = process_chunk(chunk, embedder)\n",
    "        save_progress(processed_chunk, i // chunk_size, output_dir)\n",
    "\n",
    "    # Merge all processed chunks\n",
    "    all_chunks = load_progress(output_dir)\n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_preprocessed is already loaded\n",
    "    chunk_size = 1000  # Adjust based on your available memory\n",
    "    output_dir = \"embedding_progress\"\n",
    "\n",
    "    result_df = create_embedding_pipeline(df, chunk_size, output_dir)\n",
    "\n",
    "    # Save the final result\n",
    "    result_df.to_pickle(\"final_embedded_df.pkl\")\n",
    "\n",
    "    print(f\"Final DataFrame shape: {result_df.shape}\")\n",
    "    print(f\"Sample embedding shape: {result_df['dense_embedding'].iloc[0].shape}\")\n",
    "    assert result_df['dense_embedding'].iloc[0].shape[0] == 384, \"Embedding dimension is not 384\"\n",
    "    print(\"Embedding dimension verified: 384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Clean the original dataset + delete the classfied examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Already done and saves as 'original_preprocessed_en.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Embeddings for the Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastembed import TextEmbedder\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class FastembedTextEmbedder:\n",
    "    def __init__(self, model: str, prefix: str, dimensions: int = 384):\n",
    "        self.embedder = TextEmbedder(model, max_length=512, dim=dimensions)\n",
    "        self.prefix = prefix\n",
    "        self.dimensions = dimensions\n",
    "\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        prefixed_texts = [f\"{self.prefix}{text}\" for text in texts]\n",
    "        embeddings = list(self.embedder.embed(prefixed_texts))\n",
    "        return np.array(embeddings)\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame, embedder: FastembedTextEmbedder) -> pd.DataFrame:\n",
    "    chunk['dense_embedding'] = list(embedder.embed(chunk['preprocessed_text'].tolist()))\n",
    "    return chunk\n",
    "\n",
    "def save_progress(chunk: pd.DataFrame, chunk_id: int, output_dir: str):\n",
    "    output_path = os.path.join(output_dir, f\"chunk_{chunk_id}.pkl\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(chunk, f)\n",
    "\n",
    "def load_progress(output_dir: str) -> List[pd.DataFrame]:\n",
    "    chunks = []\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.startswith(\"chunk_\") and filename.endswith(\".pkl\"):\n",
    "            with open(os.path.join(output_dir, filename), 'rb') as f:\n",
    "                chunks.append(pickle.load(f))\n",
    "    return chunks\n",
    "\n",
    "def create_embedding_pipeline(df: pd.DataFrame, chunk_size: int, output_dir: str) -> pd.DataFrame:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    embedder = FastembedTextEmbedder(\n",
    "        model=\"BAAI/bge-small-en-v1.5\",\n",
    "        prefix=\"Represent this sentence for searching relevant passages: \",\n",
    "        dimensions=384\n",
    "    )\n",
    "\n",
    "    # Check for existing progress\n",
    "    existing_chunks = load_progress(output_dir)\n",
    "    if existing_chunks:\n",
    "        print(f\"Found {len(existing_chunks)} existing chunks. Resuming from the last processed chunk.\")\n",
    "        df = pd.concat([chunk for chunk in existing_chunks if 'dense_embedding' in chunk.columns])\n",
    "        start_index = len(df)\n",
    "    else:\n",
    "        start_index = 0\n",
    "\n",
    "    # Process remaining data in chunks\n",
    "    for i in tqdm(range(start_index, len(df), chunk_size), desc=\"Processing chunks\"):\n",
    "        chunk = df.iloc[i:i+chunk_size].copy()\n",
    "        processed_chunk = process_chunk(chunk, embedder)\n",
    "        save_progress(processed_chunk, i // chunk_size, output_dir)\n",
    "\n",
    "    # Merge all processed chunks\n",
    "    all_chunks = load_progress(output_dir)\n",
    "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_preprocessed is already loaded\n",
    "    chunk_size = 1000  # Adjust based on your available memory\n",
    "    output_dir = \"embedding_progress\"\n",
    "\n",
    "    result_df = create_embedding_pipeline(df_preprocessed, chunk_size, output_dir)\n",
    "\n",
    "    # Save the final result\n",
    "    result_df.to_pickle(\"final_embedded_df.pkl\")\n",
    "\n",
    "    print(f\"Final DataFrame shape: {result_df.shape}\")\n",
    "    print(f\"Sample embedding shape: {result_df['dense_embedding'].iloc[0].shape}\")\n",
    "    assert result_df['dense_embedding'].iloc[0].shape[0] == 384, \"Embedding dimension is not 384\"\n",
    "    print(\"Embedding dimension verified: 384\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
