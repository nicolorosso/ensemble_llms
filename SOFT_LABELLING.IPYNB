{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ST TRY WITH SOFT LABELLING and Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.read_parquet(r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c.dense_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.rename({'embedding': 'dense_embedding'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>dense_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>Sea levels aren't rising or anything. Amazing ...</td>\n",
       "      <td>Sea levels rising Amazing photos Venice underw...</td>\n",
       "      <td>[-0.07484601, 0.025625845, 0.09780532, 0.05052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224</td>\n",
       "      <td>I just dumped a bucket of room temperature wat...</td>\n",
       "      <td>I dumped bucket room temperature water head cl...</td>\n",
       "      <td>[-0.01502552, 0.026903296, 0.10912616, -0.0106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>509</td>\n",
       "      <td>interested in taking action on #ClimateChange?...</td>\n",
       "      <td>interested taking action ClimateChange help fu...</td>\n",
       "      <td>[-0.023086004, 0.025529342, 0.10543756, 0.0440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>785</td>\n",
       "      <td>\"But I'm already using #5G on my AT&amp;amp;T phon...</td>\n",
       "      <td>But I AT amp T phone Every customer duped AT a...</td>\n",
       "      <td>[-0.045471866, 0.009822818, 0.058821313, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>985</td>\n",
       "      <td>In the latest @taprootyeg story, @stephencooke...</td>\n",
       "      <td>In latest story looks access means Edmonton re...</td>\n",
       "      <td>[0.028260324, 0.02326092, -0.0033834244, 0.022...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id                                               text  \\\n",
       "0         22  Sea levels aren't rising or anything. Amazing ...   \n",
       "1        224  I just dumped a bucket of room temperature wat...   \n",
       "2        509  interested in taking action on #ClimateChange?...   \n",
       "3        785  \"But I'm already using #5G on my AT&amp;T phon...   \n",
       "4        985  In the latest @taprootyeg story, @stephencooke...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  Sea levels rising Amazing photos Venice underw...   \n",
       "1  I dumped bucket room temperature water head cl...   \n",
       "2  interested taking action ClimateChange help fu...   \n",
       "3  But I AT amp T phone Every customer duped AT a...   \n",
       "4  In latest story looks access means Edmonton re...   \n",
       "\n",
       "                                     dense_embedding  \n",
       "0  [-0.07484601, 0.025625845, 0.09780532, 0.05052...  \n",
       "1  [-0.01502552, 0.026903296, 0.10912616, -0.0106...  \n",
       "2  [-0.023086004, 0.025529342, 0.10543756, 0.0440...  \n",
       "3  [-0.045471866, 0.009822818, 0.058821313, -0.05...  \n",
       "4  [0.028260324, 0.02326092, -0.0033834244, 0.022...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_parquet(r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 12:40:35,014 - INFO - Initializing CatBoostActiveLearningPipeline\n",
      "2024-09-25 12:40:35,015 - INFO - Labeled data file: sbert_data_with_distance_features.csv\n",
      "2024-09-25 12:40:35,015 - INFO - Augmented data file: cleaned_proscience_da.csv\n",
      "2024-09-25 12:40:35,016 - INFO - Unlabeled data file: C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet\n",
      "2024-09-25 12:40:35,017 - INFO - Embedding column: dense_embedding\n",
      "2024-09-25 12:40:35,018 - INFO - Label column: cleaned_classification\n",
      "2024-09-25 12:40:35,019 - INFO - Test size: 0.2\n",
      "2024-09-25 12:40:35,020 - INFO - Initial labeled ratio: 0.3\n",
      "2024-09-25 12:40:35,021 - INFO - Random state: 42\n",
      "2024-09-25 12:40:35,021 - INFO - Batch size: 1000\n",
      "2024-09-25 12:40:35,022 - INFO - Loading and preprocessing data...\n",
      "2024-09-25 12:40:47,626 - INFO - Labeled data shape: (79763, 13)\n",
      "2024-09-25 12:40:48,662 - INFO - Augmented data shape: (5378, 4)\n",
      "2024-09-25 12:40:52,661 - INFO - Unlabeled data shape: (412879, 384)\n",
      "2024-09-25 12:40:52,662 - INFO - FAISS index size: 412879\n",
      "2024-09-25 12:40:52,662 - INFO - Data loaded and preprocessed in 17.64 seconds\n",
      "2024-09-25 12:40:52,666 - INFO - Starting CatBoost active learning pipeline\n",
      "2024-09-25 12:40:52,667 - INFO - Preparing data for active learning\n",
      "2024-09-25 12:40:53,183 - INFO - Initial labeled set shape: (20433, 384)\n",
      "2024-09-25 12:40:53,184 - INFO - Pool set shape: (47679, 384)\n",
      "2024-09-25 12:40:53,185 - INFO - Test set shape: (17029, 384)\n",
      "2024-09-25 12:40:53,219 - INFO - Starting active learning with CatBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0694043\ttotal: 287ms\tremaining: 4m 47s\n",
      "100:\tlearn: 0.6936732\ttotal: 9.32s\tremaining: 1m 22s\n",
      "200:\tlearn: 0.5843712\ttotal: 20.8s\tremaining: 1m 22s\n",
      "300:\tlearn: 0.5123744\ttotal: 34.9s\tremaining: 1m 20s\n",
      "400:\tlearn: 0.4579681\ttotal: 48.8s\tremaining: 1m 12s\n",
      "500:\tlearn: 0.4133818\ttotal: 1m 1s\tremaining: 1m 1s\n",
      "600:\tlearn: 0.3757239\ttotal: 1m 14s\tremaining: 49.8s\n",
      "700:\tlearn: 0.3440428\ttotal: 1m 28s\tremaining: 37.8s\n",
      "800:\tlearn: 0.3190920\ttotal: 1m 43s\tremaining: 25.7s\n",
      "900:\tlearn: 0.2952294\ttotal: 1m 57s\tremaining: 12.9s\n",
      "999:\tlearn: 0.2750112\ttotal: 2m 10s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718ace5969db4e1da978601e586cbf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Active Learning Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0706507\ttotal: 135ms\tremaining: 2m 15s\n",
      "100:\tlearn: 0.7081786\ttotal: 15.2s\tremaining: 2m 15s\n",
      "200:\tlearn: 0.6038808\ttotal: 29.1s\tremaining: 1m 55s\n",
      "300:\tlearn: 0.5342437\ttotal: 42.9s\tremaining: 1m 39s\n",
      "400:\tlearn: 0.4794415\ttotal: 56.7s\tremaining: 1m 24s\n",
      "500:\tlearn: 0.4335197\ttotal: 1m 10s\tremaining: 1m 10s\n",
      "600:\tlearn: 0.3960181\ttotal: 1m 24s\tremaining: 55.9s\n",
      "700:\tlearn: 0.3645134\ttotal: 1m 38s\tremaining: 41.9s\n",
      "800:\tlearn: 0.3364766\ttotal: 1m 52s\tremaining: 28s\n",
      "900:\tlearn: 0.3118957\ttotal: 2m 10s\tremaining: 14.3s\n",
      "999:\tlearn: 0.2910103\ttotal: 2m 28s\tremaining: 0us\n",
      "0:\tlearn: 1.0702972\ttotal: 223ms\tremaining: 3m 43s\n",
      "100:\tlearn: 0.7226446\ttotal: 23.8s\tremaining: 3m 32s\n",
      "200:\tlearn: 0.6202382\ttotal: 46.2s\tremaining: 3m 3s\n",
      "300:\tlearn: 0.5499214\ttotal: 1m 5s\tremaining: 2m 32s\n",
      "400:\tlearn: 0.4957315\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "500:\tlearn: 0.4507657\ttotal: 1m 43s\tremaining: 1m 43s\n",
      "600:\tlearn: 0.4133899\ttotal: 2m\tremaining: 1m 20s\n",
      "700:\tlearn: 0.3804636\ttotal: 2m 17s\tremaining: 58.7s\n",
      "800:\tlearn: 0.3524302\ttotal: 2m 33s\tremaining: 38.1s\n",
      "900:\tlearn: 0.3267593\ttotal: 2m 49s\tremaining: 18.6s\n",
      "999:\tlearn: 0.3051785\ttotal: 3m 4s\tremaining: 0us\n",
      "0:\tlearn: 1.0720374\ttotal: 169ms\tremaining: 2m 48s\n",
      "100:\tlearn: 0.7345357\ttotal: 17.3s\tremaining: 2m 34s\n",
      "200:\tlearn: 0.6342942\ttotal: 33.2s\tremaining: 2m 12s\n",
      "300:\tlearn: 0.5651089\ttotal: 49.2s\tremaining: 1m 54s\n",
      "400:\tlearn: 0.5118751\ttotal: 1m 6s\tremaining: 1m 38s\n",
      "500:\tlearn: 0.4653368\ttotal: 1m 21s\tremaining: 1m 21s\n",
      "600:\tlearn: 0.4248224\ttotal: 1m 36s\tremaining: 1m 3s\n",
      "700:\tlearn: 0.3912882\ttotal: 1m 50s\tremaining: 47.2s\n",
      "800:\tlearn: 0.3621113\ttotal: 2m 6s\tremaining: 31.3s\n",
      "900:\tlearn: 0.3378113\ttotal: 2m 24s\tremaining: 15.9s\n",
      "999:\tlearn: 0.3159674\ttotal: 2m 40s\tremaining: 0us\n",
      "0:\tlearn: 1.0738565\ttotal: 179ms\tremaining: 2m 58s\n",
      "100:\tlearn: 0.7464519\ttotal: 17.8s\tremaining: 2m 38s\n",
      "200:\tlearn: 0.6497973\ttotal: 33.6s\tremaining: 2m 13s\n",
      "300:\tlearn: 0.5801642\ttotal: 49.6s\tremaining: 1m 55s\n",
      "400:\tlearn: 0.5253739\ttotal: 1m 5s\tremaining: 1m 38s\n",
      "500:\tlearn: 0.4802258\ttotal: 1m 22s\tremaining: 1m 22s\n",
      "600:\tlearn: 0.4413914\ttotal: 1m 39s\tremaining: 1m 6s\n",
      "700:\tlearn: 0.4070433\ttotal: 1m 54s\tremaining: 48.6s\n",
      "800:\tlearn: 0.3781491\ttotal: 2m 8s\tremaining: 31.9s\n",
      "900:\tlearn: 0.3534525\ttotal: 2m 22s\tremaining: 15.7s\n",
      "999:\tlearn: 0.3314778\ttotal: 2m 37s\tremaining: 0us\n",
      "0:\tlearn: 1.0723970\ttotal: 172ms\tremaining: 2m 52s\n",
      "100:\tlearn: 0.7557821\ttotal: 19.9s\tremaining: 2m 56s\n",
      "200:\tlearn: 0.6610578\ttotal: 41s\tremaining: 2m 43s\n",
      "300:\tlearn: 0.5931629\ttotal: 1m 1s\tremaining: 2m 22s\n",
      "400:\tlearn: 0.5398181\ttotal: 1m 20s\tremaining: 2m\n",
      "500:\tlearn: 0.4944789\ttotal: 1m 40s\tremaining: 1m 39s\n",
      "600:\tlearn: 0.4542625\ttotal: 1m 58s\tremaining: 1m 18s\n",
      "700:\tlearn: 0.4205080\ttotal: 2m 17s\tremaining: 58.5s\n",
      "800:\tlearn: 0.3913100\ttotal: 2m 37s\tremaining: 39.1s\n",
      "900:\tlearn: 0.3659170\ttotal: 2m 57s\tremaining: 19.5s\n",
      "999:\tlearn: 0.3418976\ttotal: 3m 16s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 12:58:22,017 - INFO - Iteration 5/50: Accuracy = 0.7524, F1 = 0.6785, AUC-ROC = 0.8860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0730762\ttotal: 198ms\tremaining: 3m 17s\n",
      "100:\tlearn: 0.7681983\ttotal: 19.4s\tremaining: 2m 52s\n",
      "200:\tlearn: 0.6741008\ttotal: 34.8s\tremaining: 2m 18s\n",
      "300:\tlearn: 0.6078540\ttotal: 53.1s\tremaining: 2m 3s\n",
      "400:\tlearn: 0.5543302\ttotal: 1m 11s\tremaining: 1m 46s\n",
      "500:\tlearn: 0.5091633\ttotal: 1m 29s\tremaining: 1m 28s\n",
      "600:\tlearn: 0.4709927\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "700:\tlearn: 0.4365022\ttotal: 2m 3s\tremaining: 52.6s\n",
      "800:\tlearn: 0.4067177\ttotal: 2m 20s\tremaining: 35s\n",
      "900:\tlearn: 0.3806453\ttotal: 2m 38s\tremaining: 17.4s\n",
      "999:\tlearn: 0.3573030\ttotal: 2m 58s\tremaining: 0us\n",
      "0:\tlearn: 1.0748881\ttotal: 212ms\tremaining: 3m 31s\n",
      "100:\tlearn: 0.7774178\ttotal: 24.1s\tremaining: 3m 34s\n",
      "200:\tlearn: 0.6856289\ttotal: 42.3s\tremaining: 2m 48s\n",
      "300:\tlearn: 0.6188772\ttotal: 1m 1s\tremaining: 2m 21s\n",
      "400:\tlearn: 0.5659025\ttotal: 1m 22s\tremaining: 2m 3s\n",
      "500:\tlearn: 0.5209002\ttotal: 1m 43s\tremaining: 1m 43s\n",
      "600:\tlearn: 0.4816701\ttotal: 2m 2s\tremaining: 1m 21s\n",
      "700:\tlearn: 0.4480347\ttotal: 2m 20s\tremaining: 59.8s\n",
      "800:\tlearn: 0.4176828\ttotal: 2m 37s\tremaining: 39.2s\n",
      "900:\tlearn: 0.3912170\ttotal: 2m 57s\tremaining: 19.5s\n",
      "999:\tlearn: 0.3677183\ttotal: 3m 15s\tremaining: 0us\n",
      "0:\tlearn: 1.0765690\ttotal: 188ms\tremaining: 3m 7s\n",
      "100:\tlearn: 0.7850606\ttotal: 16s\tremaining: 2m 22s\n",
      "200:\tlearn: 0.6939198\ttotal: 32.7s\tremaining: 2m 9s\n",
      "300:\tlearn: 0.6293004\ttotal: 48.8s\tremaining: 1m 53s\n",
      "400:\tlearn: 0.5764163\ttotal: 1m 5s\tremaining: 1m 37s\n",
      "500:\tlearn: 0.5322084\ttotal: 1m 22s\tremaining: 1m 22s\n",
      "600:\tlearn: 0.4947966\ttotal: 1m 39s\tremaining: 1m 6s\n",
      "700:\tlearn: 0.4605075\ttotal: 1m 55s\tremaining: 49.3s\n",
      "800:\tlearn: 0.4309410\ttotal: 2m 12s\tremaining: 32.9s\n",
      "900:\tlearn: 0.4040827\ttotal: 2m 29s\tremaining: 16.4s\n",
      "999:\tlearn: 0.3806162\ttotal: 2m 45s\tremaining: 0us\n",
      "0:\tlearn: 1.0771404\ttotal: 147ms\tremaining: 2m 27s\n",
      "100:\tlearn: 0.7934948\ttotal: 15.2s\tremaining: 2m 15s\n",
      "200:\tlearn: 0.7045839\ttotal: 32.1s\tremaining: 2m 7s\n",
      "300:\tlearn: 0.6400769\ttotal: 49.9s\tremaining: 1m 55s\n",
      "400:\tlearn: 0.5867204\ttotal: 1m 8s\tremaining: 1m 42s\n",
      "500:\tlearn: 0.5429820\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "600:\tlearn: 0.5037541\ttotal: 1m 49s\tremaining: 1m 12s\n",
      "700:\tlearn: 0.4702436\ttotal: 2m 6s\tremaining: 54s\n",
      "800:\tlearn: 0.4410318\ttotal: 2m 24s\tremaining: 35.8s\n",
      "900:\tlearn: 0.4150983\ttotal: 2m 40s\tremaining: 17.7s\n",
      "999:\tlearn: 0.3914903\ttotal: 2m 56s\tremaining: 0us\n",
      "0:\tlearn: 1.0771554\ttotal: 167ms\tremaining: 2m 46s\n",
      "100:\tlearn: 0.8015249\ttotal: 18.5s\tremaining: 2m 44s\n",
      "200:\tlearn: 0.7151522\ttotal: 36.4s\tremaining: 2m 24s\n",
      "300:\tlearn: 0.6500626\ttotal: 54.6s\tremaining: 2m 6s\n",
      "400:\tlearn: 0.5979524\ttotal: 1m 12s\tremaining: 1m 48s\n",
      "500:\tlearn: 0.5531808\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "600:\tlearn: 0.5157413\ttotal: 1m 54s\tremaining: 1m 16s\n",
      "700:\tlearn: 0.4812735\ttotal: 2m 11s\tremaining: 56.2s\n",
      "800:\tlearn: 0.4515525\ttotal: 2m 29s\tremaining: 37.1s\n",
      "900:\tlearn: 0.4248469\ttotal: 2m 46s\tremaining: 18.3s\n",
      "999:\tlearn: 0.4010781\ttotal: 3m 3s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:14:30,214 - INFO - Iteration 10/50: Accuracy = 0.7520, F1 = 0.6781, AUC-ROC = 0.8848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0771502\ttotal: 174ms\tremaining: 2m 53s\n",
      "100:\tlearn: 0.8064328\ttotal: 18.2s\tremaining: 2m 42s\n",
      "200:\tlearn: 0.7235872\ttotal: 36.2s\tremaining: 2m 23s\n",
      "300:\tlearn: 0.6603970\ttotal: 55.8s\tremaining: 2m 9s\n",
      "400:\tlearn: 0.6078438\ttotal: 1m 13s\tremaining: 1m 50s\n",
      "500:\tlearn: 0.5642396\ttotal: 1m 31s\tremaining: 1m 30s\n",
      "600:\tlearn: 0.5267717\ttotal: 1m 50s\tremaining: 1m 13s\n",
      "700:\tlearn: 0.4931105\ttotal: 2m 9s\tremaining: 55.4s\n",
      "800:\tlearn: 0.4634636\ttotal: 2m 28s\tremaining: 37s\n",
      "900:\tlearn: 0.4365160\ttotal: 2m 47s\tremaining: 18.4s\n",
      "999:\tlearn: 0.4115526\ttotal: 3m 3s\tremaining: 0us\n",
      "0:\tlearn: 1.0783407\ttotal: 166ms\tremaining: 2m 45s\n",
      "100:\tlearn: 0.8139096\ttotal: 19.6s\tremaining: 2m 54s\n",
      "200:\tlearn: 0.7323787\ttotal: 37.9s\tremaining: 2m 30s\n",
      "300:\tlearn: 0.6710381\ttotal: 54.4s\tremaining: 2m 6s\n",
      "400:\tlearn: 0.6207600\ttotal: 1m 13s\tremaining: 1m 49s\n",
      "500:\tlearn: 0.5778823\ttotal: 1m 35s\tremaining: 1m 34s\n",
      "600:\tlearn: 0.5411553\ttotal: 1m 55s\tremaining: 1m 16s\n",
      "700:\tlearn: 0.5058850\ttotal: 2m 14s\tremaining: 57.2s\n",
      "800:\tlearn: 0.4761674\ttotal: 2m 36s\tremaining: 39s\n",
      "900:\tlearn: 0.4496263\ttotal: 2m 53s\tremaining: 19s\n",
      "999:\tlearn: 0.4263242\ttotal: 3m 9s\tremaining: 0us\n",
      "0:\tlearn: 1.0786679\ttotal: 166ms\tremaining: 2m 45s\n",
      "100:\tlearn: 0.8190345\ttotal: 19.7s\tremaining: 2m 55s\n",
      "200:\tlearn: 0.7366837\ttotal: 37.8s\tremaining: 2m 30s\n",
      "300:\tlearn: 0.6743831\ttotal: 57.8s\tremaining: 2m 14s\n",
      "400:\tlearn: 0.6255427\ttotal: 1m 21s\tremaining: 2m 1s\n",
      "500:\tlearn: 0.5851162\ttotal: 1m 40s\tremaining: 1m 40s\n",
      "600:\tlearn: 0.5479789\ttotal: 1m 58s\tremaining: 1m 18s\n",
      "700:\tlearn: 0.5155234\ttotal: 2m 15s\tremaining: 57.7s\n",
      "800:\tlearn: 0.4856562\ttotal: 2m 32s\tremaining: 37.8s\n",
      "900:\tlearn: 0.4600553\ttotal: 2m 49s\tremaining: 18.6s\n",
      "999:\tlearn: 0.4363411\ttotal: 3m 9s\tremaining: 0us\n",
      "0:\tlearn: 1.0792645\ttotal: 203ms\tremaining: 3m 22s\n",
      "100:\tlearn: 0.8276863\ttotal: 22.7s\tremaining: 3m 21s\n",
      "200:\tlearn: 0.7472603\ttotal: 42.4s\tremaining: 2m 48s\n",
      "300:\tlearn: 0.6847466\ttotal: 1m 2s\tremaining: 2m 24s\n",
      "400:\tlearn: 0.6356653\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "500:\tlearn: 0.5933892\ttotal: 1m 38s\tremaining: 1m 37s\n",
      "600:\tlearn: 0.5572291\ttotal: 1m 58s\tremaining: 1m 18s\n",
      "700:\tlearn: 0.5239154\ttotal: 2m 16s\tremaining: 58s\n",
      "800:\tlearn: 0.4935024\ttotal: 2m 34s\tremaining: 38.3s\n",
      "900:\tlearn: 0.4677142\ttotal: 2m 50s\tremaining: 18.8s\n",
      "999:\tlearn: 0.4453281\ttotal: 3m 8s\tremaining: 0us\n",
      "0:\tlearn: 1.0799760\ttotal: 176ms\tremaining: 2m 56s\n",
      "100:\tlearn: 0.8328210\ttotal: 16.1s\tremaining: 2m 23s\n",
      "200:\tlearn: 0.7532406\ttotal: 34.5s\tremaining: 2m 17s\n",
      "300:\tlearn: 0.6938720\ttotal: 52s\tremaining: 2m\n",
      "400:\tlearn: 0.6447364\ttotal: 1m 8s\tremaining: 1m 42s\n",
      "500:\tlearn: 0.6028788\ttotal: 1m 26s\tremaining: 1m 26s\n",
      "600:\tlearn: 0.5658849\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "700:\tlearn: 0.5345366\ttotal: 2m 3s\tremaining: 52.7s\n",
      "800:\tlearn: 0.5053391\ttotal: 2m 20s\tremaining: 34.8s\n",
      "900:\tlearn: 0.4792760\ttotal: 2m 38s\tremaining: 17.4s\n",
      "999:\tlearn: 0.4559644\ttotal: 2m 59s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:31:13,702 - INFO - Iteration 15/50: Accuracy = 0.7553, F1 = 0.6818, AUC-ROC = 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0816008\ttotal: 147ms\tremaining: 2m 27s\n",
      "100:\tlearn: 0.8393204\ttotal: 18.6s\tremaining: 2m 45s\n",
      "200:\tlearn: 0.7587596\ttotal: 37.3s\tremaining: 2m 28s\n",
      "300:\tlearn: 0.7011721\ttotal: 57.9s\tremaining: 2m 14s\n",
      "400:\tlearn: 0.6537182\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "500:\tlearn: 0.6121701\ttotal: 1m 36s\tremaining: 1m 36s\n",
      "600:\tlearn: 0.5757628\ttotal: 1m 55s\tremaining: 1m 16s\n",
      "700:\tlearn: 0.5425076\ttotal: 2m 12s\tremaining: 56.4s\n",
      "800:\tlearn: 0.5133795\ttotal: 2m 29s\tremaining: 37.1s\n",
      "900:\tlearn: 0.4876971\ttotal: 2m 47s\tremaining: 18.4s\n",
      "999:\tlearn: 0.4637570\ttotal: 3m 4s\tremaining: 0us\n",
      "0:\tlearn: 1.0807246\ttotal: 157ms\tremaining: 2m 36s\n",
      "100:\tlearn: 0.8440908\ttotal: 15.9s\tremaining: 2m 21s\n",
      "200:\tlearn: 0.7669508\ttotal: 33.5s\tremaining: 2m 13s\n",
      "300:\tlearn: 0.7100499\ttotal: 50.8s\tremaining: 1m 58s\n",
      "400:\tlearn: 0.6627244\ttotal: 1m 8s\tremaining: 1m 41s\n",
      "500:\tlearn: 0.6214245\ttotal: 1m 27s\tremaining: 1m 27s\n",
      "600:\tlearn: 0.5857818\ttotal: 1m 48s\tremaining: 1m 12s\n",
      "700:\tlearn: 0.5534521\ttotal: 2m 7s\tremaining: 54.4s\n",
      "800:\tlearn: 0.5244962\ttotal: 2m 26s\tremaining: 36.4s\n",
      "900:\tlearn: 0.4983858\ttotal: 2m 46s\tremaining: 18.3s\n",
      "999:\tlearn: 0.4751600\ttotal: 3m 4s\tremaining: 0us\n",
      "0:\tlearn: 1.0812150\ttotal: 169ms\tremaining: 2m 49s\n",
      "100:\tlearn: 0.8508712\ttotal: 17.7s\tremaining: 2m 37s\n",
      "200:\tlearn: 0.7739597\ttotal: 35.6s\tremaining: 2m 21s\n",
      "300:\tlearn: 0.7167202\ttotal: 52.4s\tremaining: 2m 1s\n",
      "400:\tlearn: 0.6707208\ttotal: 1m 9s\tremaining: 1m 43s\n",
      "500:\tlearn: 0.6313435\ttotal: 1m 26s\tremaining: 1m 26s\n",
      "600:\tlearn: 0.5955127\ttotal: 1m 44s\tremaining: 1m 9s\n",
      "700:\tlearn: 0.5624431\ttotal: 2m 1s\tremaining: 51.8s\n",
      "800:\tlearn: 0.5340457\ttotal: 2m 17s\tremaining: 34.3s\n",
      "900:\tlearn: 0.5082833\ttotal: 2m 34s\tremaining: 17s\n",
      "999:\tlearn: 0.4851122\ttotal: 2m 51s\tremaining: 0us\n",
      "0:\tlearn: 1.0813917\ttotal: 169ms\tremaining: 2m 48s\n",
      "100:\tlearn: 0.8555099\ttotal: 17.8s\tremaining: 2m 38s\n",
      "200:\tlearn: 0.7804948\ttotal: 36.2s\tremaining: 2m 24s\n",
      "300:\tlearn: 0.7231186\ttotal: 54.3s\tremaining: 2m 6s\n",
      "400:\tlearn: 0.6750360\ttotal: 1m 12s\tremaining: 1m 47s\n",
      "500:\tlearn: 0.6357059\ttotal: 1m 29s\tremaining: 1m 28s\n",
      "600:\tlearn: 0.5999468\ttotal: 1m 46s\tremaining: 1m 10s\n",
      "700:\tlearn: 0.5692654\ttotal: 2m 3s\tremaining: 52.7s\n",
      "800:\tlearn: 0.5401715\ttotal: 2m 21s\tremaining: 35.2s\n",
      "900:\tlearn: 0.5145450\ttotal: 2m 38s\tremaining: 17.4s\n",
      "999:\tlearn: 0.4913509\ttotal: 2m 55s\tremaining: 0us\n",
      "0:\tlearn: 1.0818740\ttotal: 172ms\tremaining: 2m 51s\n",
      "100:\tlearn: 0.8613412\ttotal: 16.1s\tremaining: 2m 23s\n",
      "200:\tlearn: 0.7883495\ttotal: 34.7s\tremaining: 2m 18s\n",
      "300:\tlearn: 0.7312796\ttotal: 53.6s\tremaining: 2m 4s\n",
      "400:\tlearn: 0.6834913\ttotal: 1m 13s\tremaining: 1m 49s\n",
      "500:\tlearn: 0.6441750\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "600:\tlearn: 0.6095712\ttotal: 1m 52s\tremaining: 1m 14s\n",
      "700:\tlearn: 0.5791260\ttotal: 2m 11s\tremaining: 56.2s\n",
      "800:\tlearn: 0.5511733\ttotal: 2m 33s\tremaining: 38.2s\n",
      "900:\tlearn: 0.5259994\ttotal: 2m 54s\tremaining: 19.1s\n",
      "999:\tlearn: 0.5032093\ttotal: 3m 15s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:47:35,276 - INFO - Iteration 20/50: Accuracy = 0.7493, F1 = 0.6751, AUC-ROC = 0.8843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0819914\ttotal: 164ms\tremaining: 2m 43s\n",
      "100:\tlearn: 0.8653197\ttotal: 19.8s\tremaining: 2m 55s\n",
      "200:\tlearn: 0.7917901\ttotal: 38.7s\tremaining: 2m 33s\n",
      "300:\tlearn: 0.7365635\ttotal: 57.9s\tremaining: 2m 14s\n",
      "400:\tlearn: 0.6900355\ttotal: 1m 16s\tremaining: 1m 54s\n",
      "500:\tlearn: 0.6511686\ttotal: 1m 34s\tremaining: 1m 34s\n",
      "600:\tlearn: 0.6162716\ttotal: 1m 53s\tremaining: 1m 15s\n",
      "700:\tlearn: 0.5846882\ttotal: 2m 13s\tremaining: 56.8s\n",
      "800:\tlearn: 0.5570097\ttotal: 2m 34s\tremaining: 38.4s\n",
      "900:\tlearn: 0.5312911\ttotal: 2m 54s\tremaining: 19.2s\n",
      "999:\tlearn: 0.5082043\ttotal: 3m 14s\tremaining: 0us\n",
      "0:\tlearn: 1.0822802\ttotal: 333ms\tremaining: 5m 32s\n",
      "100:\tlearn: 0.8705503\ttotal: 26.5s\tremaining: 3m 55s\n",
      "200:\tlearn: 0.7974928\ttotal: 46.7s\tremaining: 3m 5s\n",
      "300:\tlearn: 0.7414811\ttotal: 1m 7s\tremaining: 2m 37s\n",
      "400:\tlearn: 0.6955372\ttotal: 1m 27s\tremaining: 2m 10s\n",
      "500:\tlearn: 0.6563021\ttotal: 1m 46s\tremaining: 1m 45s\n",
      "600:\tlearn: 0.6218835\ttotal: 2m 6s\tremaining: 1m 23s\n",
      "700:\tlearn: 0.5918484\ttotal: 2m 26s\tremaining: 1m 2s\n",
      "800:\tlearn: 0.5642104\ttotal: 2m 44s\tremaining: 40.9s\n",
      "900:\tlearn: 0.5388804\ttotal: 3m 3s\tremaining: 20.1s\n",
      "999:\tlearn: 0.5159886\ttotal: 3m 22s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:54:46,098 - INFO - Early stopping at iteration 22 due to no improvement\n",
      "2024-09-25 13:54:46,102 - INFO - Active learning completed in 4432.88 seconds\n",
      "2024-09-25 13:54:46,103 - INFO - Final performance: Accuracy = 0.7524, F1 = 0.6772, AUC-ROC = 0.8853\n",
      "2024-09-25 13:54:47,769 - INFO - CatBoost Active Learning Pipeline completed in 4435.10 seconds\n",
      "2024-09-25 13:54:47,770 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " antiscience       0.67      0.73      0.70      5337\n",
      "     neutral       0.87      0.82      0.85      9541\n",
      "  proscience       0.47      0.50      0.48      2151\n",
      "\n",
      "    accuracy                           0.75     17029\n",
      "   macro avg       0.67      0.68      0.68     17029\n",
      "weighted avg       0.76      0.75      0.76     17029\n",
      "\n",
      "2024-09-25 13:54:48,269 - INFO - Results plot saved as 'catboost_active_learning_results.png'\n",
      "2024-09-25 13:54:48,491 - INFO - Confusion matrix saved as 'catboost_confusion_matrix.png'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " antiscience       0.67      0.73      0.70      5337\n",
      "     neutral       0.87      0.82      0.85      9541\n",
      "  proscience       0.47      0.50      0.48      2151\n",
      "\n",
      "    accuracy                           0.75     17029\n",
      "   macro avg       0.67      0.68      0.68     17029\n",
      "weighted avg       0.76      0.75      0.76     17029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CatBoostActiveLearningPipeline:\n",
    "    def __init__(self, labeled_data_file: str, augmented_data_file: str, unlabeled_data_file: str, \n",
    "                 embedding_column: str, label_column: str, test_size: float = 0.2, \n",
    "                 initial_labeled_ratio: float = 0.3, random_state: int = 42, batch_size: int = 1000):\n",
    "        self.labeled_data_file = labeled_data_file\n",
    "        self.augmented_data_file = augmented_data_file\n",
    "        self.unlabeled_data_file = unlabeled_data_file\n",
    "        self.embedding_column = embedding_column\n",
    "        self.label_column = label_column\n",
    "        self.test_size = test_size\n",
    "        self.initial_labeled_ratio = initial_labeled_ratio\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "        self._log_init_info()\n",
    "        self._load_and_preprocess_data()\n",
    "        \n",
    "    def _log_init_info(self) -> None:\n",
    "        logger.info(\"Initializing CatBoostActiveLearningPipeline\")\n",
    "        logger.info(f\"Labeled data file: {self.labeled_data_file}\")\n",
    "        logger.info(f\"Augmented data file: {self.augmented_data_file}\")\n",
    "        logger.info(f\"Unlabeled data file: {self.unlabeled_data_file}\")\n",
    "        logger.info(f\"Embedding column: {self.embedding_column}\")\n",
    "        logger.info(f\"Label column: {self.label_column}\")\n",
    "        logger.info(f\"Test size: {self.test_size}\")\n",
    "        logger.info(f\"Initial labeled ratio: {self.initial_labeled_ratio}\")\n",
    "        logger.info(f\"Random state: {self.random_state}\")\n",
    "        logger.info(f\"Batch size: {self.batch_size}\")\n",
    "\n",
    "    def _process_embeddings(self, df):\n",
    "        def process_embedding(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip('[]')\n",
    "                try:\n",
    "                    return np.array([float(i) for i in x.split(',')])\n",
    "                except ValueError:\n",
    "                    return np.array([float(i) for i in x.split()])\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                return x\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected embedding format: {type(x)}\")\n",
    "\n",
    "        df[self.embedding_column] = df[self.embedding_column].apply(process_embedding)\n",
    "        return df\n",
    "\n",
    "    def _load_and_preprocess_data(self) -> None:\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Load labeled data\n",
    "        self.labeled_data = pd.read_csv(self.labeled_data_file)\n",
    "        self.labeled_data = self._process_embeddings(self.labeled_data)\n",
    "        logger.info(f\"Labeled data shape: {self.labeled_data.shape}\")\n",
    "        \n",
    "        # Load augmented data\n",
    "        self.augmented_data = pd.read_csv(self.augmented_data_file)\n",
    "        self.augmented_data = self._process_embeddings(self.augmented_data)\n",
    "        logger.info(f\"Augmented data shape: {self.augmented_data.shape}\")\n",
    "        \n",
    "        # Combine labeled and augmented data for label encoding\n",
    "        all_data = pd.concat([self.labeled_data, self.augmented_data])\n",
    "        \n",
    "        # Encode labels\n",
    "        self.le.fit(all_data[self.label_column])\n",
    "        self.labeled_data['encoded_label'] = self.le.transform(self.labeled_data[self.label_column])\n",
    "        self.augmented_data['encoded_label'] = self.le.transform(self.augmented_data[self.label_column])\n",
    "        \n",
    "        # Initialize FAISS index\n",
    "        self.dimension = len(self.labeled_data[self.embedding_column].iloc[0])\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        \n",
    "        # Load unlabeled data and add to FAISS index\n",
    "        self.unlabeled_data = []\n",
    "        parquet_file = pq.ParquetFile(self.unlabeled_data_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.batch_size, columns=[self.embedding_column]):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            df_chunk = self._process_embeddings(df_chunk)\n",
    "            embeddings = np.stack(df_chunk[self.embedding_column].values)\n",
    "            self.index.add(embeddings)\n",
    "            self.unlabeled_data.append(embeddings)\n",
    "        \n",
    "        self.unlabeled_data = np.vstack(self.unlabeled_data)\n",
    "        logger.info(f\"Unlabeled data shape: {self.unlabeled_data.shape}\")\n",
    "        logger.info(f\"FAISS index size: {self.index.ntotal}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Data loaded and preprocessed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        logger.info(\"Preparing data for active learning\")\n",
    "        \n",
    "        # Combine labeled and augmented data\n",
    "        X = np.vstack([\n",
    "            np.stack(self.labeled_data[self.embedding_column].values),\n",
    "            np.stack(self.augmented_data[self.embedding_column].values)\n",
    "        ])\n",
    "        y = np.concatenate([\n",
    "            self.labeled_data['encoded_label'].values,\n",
    "            self.augmented_data['encoded_label'].values\n",
    "        ])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=y)\n",
    "        \n",
    "        X_initial, X_pool, y_initial, y_pool = train_test_split(\n",
    "            X_train, y_train, train_size=self.initial_labeled_ratio, \n",
    "            random_state=self.random_state, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Initial labeled set shape: {X_initial.shape}\")\n",
    "        logger.info(f\"Pool set shape: {X_pool.shape}\")\n",
    "        logger.info(f\"Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        return X_initial, y_initial, X_pool, y_pool, X_test, y_test\n",
    "\n",
    "    def train_and_evaluate(self, learner, X_test, y_test):\n",
    "        y_pred = learner.predict(X_test)\n",
    "        y_pred_proba = learner.predict_proba(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "        \n",
    "        return accuracy, f1, auc_roc\n",
    "\n",
    "    def find_similar_samples(self, query_embeddings, k=5):\n",
    "        distances, indices = self.index.search(query_embeddings, k)\n",
    "        return distances, indices\n",
    "\n",
    "    def soft_labeling(self, uncertain_samples, uncertain_labels, n_similar=5):\n",
    "        distances, similar_indices = self.find_similar_samples(uncertain_samples, k=n_similar)\n",
    "        \n",
    "        soft_labels = np.zeros((len(similar_indices.flatten()), len(self.le.classes_)))\n",
    "        for i, (dist, indices) in enumerate(zip(distances, similar_indices)):\n",
    "            weights = np.exp(-dist)\n",
    "            soft_labels[i*n_similar:(i+1)*n_similar] = weights[:, np.newaxis] * (uncertain_labels[i] == np.arange(len(self.le.classes_)))\n",
    "        soft_labels /= soft_labels.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return self.unlabeled_data[similar_indices.flatten()], soft_labels\n",
    "\n",
    "    def train_with_active_learning(self, X_initial, y_initial, X_pool, y_pool, X_test, y_test, \n",
    "                                   n_queries=50, n_instances_per_query=100, n_similar=5):\n",
    "        logger.info(\"Starting active learning with CatBoost\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        catboost_model = CatBoostClassifier(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            loss_function='MultiClass',\n",
    "            random_seed=self.random_state,\n",
    "            eval_metric='MultiClass',\n",
    "            auto_class_weights='Balanced',\n",
    "            verbose=100\n",
    "        )\n",
    "\n",
    "        learner = ActiveLearner(\n",
    "            estimator=catboost_model,\n",
    "            X_training=X_initial,\n",
    "            y_training=y_initial,\n",
    "            query_strategy=entropy_sampling\n",
    "        )\n",
    "\n",
    "        performance_history = []\n",
    "        best_accuracy = 0\n",
    "        iterations_without_improvement = 0\n",
    "        max_iterations_without_improvement = 10\n",
    "\n",
    "        for iteration in tqdm(range(n_queries), desc=\"Active Learning Progress\"):\n",
    "            query_idx, _ = learner.query(X_pool, n_instances=n_instances_per_query)\n",
    "            \n",
    "            uncertain_samples = X_pool[query_idx]\n",
    "            uncertain_labels = y_pool[query_idx]\n",
    "            \n",
    "            # Get similar samples and their soft labels\n",
    "            similar_samples, soft_labels = self.soft_labeling(uncertain_samples, uncertain_labels, n_similar)\n",
    "            \n",
    "            # Combine uncertain samples and similar samples\n",
    "            X_to_teach = np.vstack([uncertain_samples, similar_samples])\n",
    "            y_to_teach = np.concatenate([uncertain_labels, np.argmax(soft_labels, axis=1)])\n",
    "            \n",
    "            # Teach the model\n",
    "            learner.teach(X_to_teach, y_to_teach)\n",
    "            \n",
    "            # Remove queried samples from the pool\n",
    "            X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "            y_pool = np.delete(y_pool, query_idx)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            accuracy, f1, auc_roc = self.train_and_evaluate(learner, X_test, y_test)\n",
    "            performance_history.append((accuracy, f1, auc_roc))\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                iterations_without_improvement = 0\n",
    "                self.best_model = learner\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "\n",
    "            if (iteration + 1) % 5 == 0:\n",
    "                logger.info(f\"Iteration {iteration + 1}/{n_queries}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "            if iterations_without_improvement >= max_iterations_without_improvement:\n",
    "                logger.info(f\"Early stopping at iteration {iteration + 1} due to no improvement\")\n",
    "                break\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Active learning completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Final performance: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "        return performance_history, learner\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        logger.info(\"Starting CatBoost active learning pipeline\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        X_initial, y_initial, X_pool, y_pool, X_test, y_test = self.prepare_data()\n",
    "        performance_history, final_model = self.train_with_active_learning(\n",
    "            X_initial, y_initial, X_pool, y_pool, X_test, y_test\n",
    "        )\n",
    "\n",
    "        y_pred = final_model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred, target_names=self.le.classes_)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"CatBoost Active Learning Pipeline completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(\"Classification Report:\\n\" + class_report)\n",
    "\n",
    "        self.plot_results(performance_history)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "\n",
    "        return performance_history, cm, class_report, final_model\n",
    "\n",
    "    def plot_results(self, performance_history):\n",
    "        accuracies, f1_scores, auc_rocs = zip(*performance_history)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(accuracies, label='Accuracy')\n",
    "        plt.plot(f1_scores, label='F1 Score')\n",
    "        plt.plot(auc_rocs, label='AUC-ROC')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Model Performance over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('catboost_active_learning_results.png')\n",
    "        logger.info(\"Results plot saved as 'catboost_active_learning_results.png'\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('catboost_confusion_matrix.png')\n",
    "        logger.info(\"Confusion matrix saved as 'catboost_confusion_matrix.png'\")\n",
    "        plt.close()\n",
    "\n",
    "# Usage\n",
    "labeled_data_file = 'sbert_data_with_distance_features.csv'\n",
    "augmented_data_file = 'cleaned_proscience_da.csv'\n",
    "unlabeled_data_file = r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet'\n",
    "\n",
    "active_learning_pipeline = CatBoostActiveLearningPipeline(labeled_data_file, augmented_data_file, unlabeled_data_file, \"dense_embedding\", \"cleaned_classification\")\n",
    "performance_history, cm, class_report, final_model = active_learning_pipeline.run_pipeline()\n",
    "\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NN with consitency loss (with mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 15:50:05,405 - INFO - Initializing PyTorchActiveLearningPipeline\n",
      "2024-09-26 15:50:05,406 - INFO - Labeled data file: sbert_data_with_distance_features.csv\n",
      "2024-09-26 15:50:05,407 - INFO - Augmented data file: cleaned_proscience_da.csv\n",
      "2024-09-26 15:50:05,408 - INFO - Unlabeled data file: C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet\n",
      "2024-09-26 15:50:05,408 - INFO - Embedding column: dense_embedding\n",
      "2024-09-26 15:50:05,409 - INFO - Label column: cleaned_classification\n",
      "2024-09-26 15:50:05,409 - INFO - Test size: 0.2\n",
      "2024-09-26 15:50:05,410 - INFO - Initial labeled ratio: 0.3\n",
      "2024-09-26 15:50:05,411 - INFO - Random state: 42\n",
      "2024-09-26 15:50:05,411 - INFO - Batch size: 64\n",
      "2024-09-26 15:50:05,412 - INFO - Loading and preprocessing data...\n",
      "2024-09-26 15:50:18,163 - INFO - Labeled data shape: (79763, 13)\n",
      "2024-09-26 15:50:19,238 - INFO - Augmented data shape: (5378, 4)\n",
      "2024-09-26 15:50:26,313 - INFO - Unlabeled data shape: (412879, 384)\n",
      "2024-09-26 15:50:26,831 - INFO - FAISS index size: 412879\n",
      "2024-09-26 15:50:26,832 - INFO - Data loaded and preprocessed in 21.42 seconds\n",
      "2024-09-26 15:50:26,843 - INFO - Starting PyTorch active learning pipeline\n",
      "2024-09-26 15:50:26,844 - INFO - Preparing data for active learning\n",
      "2024-09-26 15:50:27,741 - INFO - Initial labeled set shape: (25542, 384)\n",
      "2024-09-26 15:50:27,942 - INFO - Pool set shape: (47679, 384)\n",
      "2024-09-26 15:50:27,947 - INFO - Test set shape: (11920, 384)\n",
      "2024-09-26 15:50:28,143 - INFO - Starting active learning with PyTorch and consistency regularization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7261eefb7340b88b16a46df4862962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Active Learning Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 15:50:28,289 - INFO - Query Round 1/50\n",
      "2024-09-26 15:50:29,964 - INFO - Epoch [1/5], Loss: 0.6735\n",
      "2024-09-26 15:50:31,470 - INFO - Epoch [2/5], Loss: 0.6060\n",
      "2024-09-26 15:50:33,003 - INFO - Epoch [3/5], Loss: 0.5865\n",
      "2024-09-26 15:50:34,540 - INFO - Epoch [4/5], Loss: 0.5744\n",
      "2024-09-26 15:50:36,063 - INFO - Epoch [5/5], Loss: 0.5623\n",
      "2024-09-26 15:50:44,336 - INFO - Epoch [1/5], Loss: 0.5548\n",
      "2024-09-26 15:50:47,958 - INFO - Epoch [2/5], Loss: 0.5448\n",
      "2024-09-26 15:50:51,429 - INFO - Epoch [3/5], Loss: 0.5333\n",
      "2024-09-26 15:50:55,015 - INFO - Epoch [4/5], Loss: 0.5215\n",
      "2024-09-26 15:50:58,629 - INFO - Epoch [5/5], Loss: 0.5126\n",
      "2024-09-26 15:50:58,737 - INFO - Query Round 2/50\n",
      "2024-09-26 15:51:00,502 - INFO - Epoch [1/5], Loss: 0.5005\n",
      "2024-09-26 15:51:02,724 - INFO - Epoch [2/5], Loss: 0.4899\n",
      "2024-09-26 15:51:04,826 - INFO - Epoch [3/5], Loss: 0.4780\n",
      "2024-09-26 15:51:07,177 - INFO - Epoch [4/5], Loss: 0.4682\n",
      "2024-09-26 15:51:09,435 - INFO - Epoch [5/5], Loss: 0.4581\n",
      "2024-09-26 15:51:19,779 - INFO - Epoch [1/5], Loss: 0.4531\n",
      "2024-09-26 15:51:24,950 - INFO - Epoch [2/5], Loss: 0.4421\n",
      "2024-09-26 15:51:30,258 - INFO - Epoch [3/5], Loss: 0.4316\n",
      "2024-09-26 15:51:35,886 - INFO - Epoch [4/5], Loss: 0.4219\n",
      "2024-09-26 15:51:42,556 - INFO - Epoch [5/5], Loss: 0.4103\n",
      "2024-09-26 15:51:42,619 - INFO - Query Round 3/50\n",
      "2024-09-26 15:51:45,841 - INFO - Epoch [1/5], Loss: 0.4031\n",
      "2024-09-26 15:51:48,010 - INFO - Epoch [2/5], Loss: 0.3934\n",
      "2024-09-26 15:51:50,077 - INFO - Epoch [3/5], Loss: 0.3853\n",
      "2024-09-26 15:51:52,798 - INFO - Epoch [4/5], Loss: 0.3751\n",
      "2024-09-26 15:51:55,088 - INFO - Epoch [5/5], Loss: 0.3657\n",
      "2024-09-26 15:52:13,682 - INFO - Epoch [1/5], Loss: 0.3620\n",
      "2024-09-26 15:52:18,605 - INFO - Epoch [2/5], Loss: 0.3531\n",
      "2024-09-26 15:52:24,409 - INFO - Epoch [3/5], Loss: 0.3478\n",
      "2024-09-26 15:52:29,556 - INFO - Epoch [4/5], Loss: 0.3398\n",
      "2024-09-26 15:52:34,629 - INFO - Epoch [5/5], Loss: 0.3314\n",
      "2024-09-26 15:52:34,788 - INFO - Query Round 4/50\n",
      "2024-09-26 15:52:36,955 - INFO - Epoch [1/5], Loss: 0.3244\n",
      "2024-09-26 15:52:38,995 - INFO - Epoch [2/5], Loss: 0.3159\n",
      "2024-09-26 15:52:42,977 - INFO - Epoch [3/5], Loss: 0.3121\n",
      "2024-09-26 15:52:45,562 - INFO - Epoch [4/5], Loss: 0.3054\n",
      "2024-09-26 15:52:47,743 - INFO - Epoch [5/5], Loss: 0.2964\n",
      "2024-09-26 15:53:01,730 - INFO - Epoch [1/5], Loss: 0.2955\n",
      "2024-09-26 15:53:06,484 - INFO - Epoch [2/5], Loss: 0.2929\n",
      "2024-09-26 15:53:11,754 - INFO - Epoch [3/5], Loss: 0.2859\n",
      "2024-09-26 15:53:17,192 - INFO - Epoch [4/5], Loss: 0.2826\n",
      "2024-09-26 15:53:22,439 - INFO - Epoch [5/5], Loss: 0.2783\n",
      "2024-09-26 15:53:22,639 - INFO - Query Round 5/50\n",
      "2024-09-26 15:53:24,582 - INFO - Epoch [1/5], Loss: 0.2707\n",
      "2024-09-26 15:53:26,631 - INFO - Epoch [2/5], Loss: 0.2661\n",
      "2024-09-26 15:53:28,580 - INFO - Epoch [3/5], Loss: 0.2604\n",
      "2024-09-26 15:53:30,577 - INFO - Epoch [4/5], Loss: 0.2543\n",
      "2024-09-26 15:53:32,509 - INFO - Epoch [5/5], Loss: 0.2559\n",
      "2024-09-26 15:53:41,530 - INFO - Epoch [1/5], Loss: 0.2523\n",
      "2024-09-26 15:53:45,947 - INFO - Epoch [2/5], Loss: 0.2466\n",
      "2024-09-26 15:53:50,071 - INFO - Epoch [3/5], Loss: 0.2415\n",
      "2024-09-26 15:54:06,348 - INFO - Epoch [4/5], Loss: 0.2416\n",
      "2024-09-26 15:54:10,582 - INFO - Epoch [5/5], Loss: 0.2355\n",
      "2024-09-26 15:54:10,648 - INFO - Iteration 5/50: Accuracy = 0.7818, F1 = 0.7015, AUC-ROC = 0.8987\n",
      "2024-09-26 15:54:10,650 - INFO - Query Round 6/50\n",
      "2024-09-26 15:54:12,804 - INFO - Epoch [1/5], Loss: 0.2354\n",
      "2024-09-26 15:54:14,964 - INFO - Epoch [2/5], Loss: 0.2244\n",
      "2024-09-26 15:54:17,234 - INFO - Epoch [3/5], Loss: 0.2212\n",
      "2024-09-26 15:54:19,428 - INFO - Epoch [4/5], Loss: 0.2209\n",
      "2024-09-26 15:54:21,630 - INFO - Epoch [5/5], Loss: 0.2174\n",
      "2024-09-26 15:54:31,375 - INFO - Epoch [1/5], Loss: 0.2191\n",
      "2024-09-26 15:54:37,441 - INFO - Epoch [2/5], Loss: 0.2194\n",
      "2024-09-26 15:54:42,982 - INFO - Epoch [3/5], Loss: 0.2119\n",
      "2024-09-26 15:54:48,819 - INFO - Epoch [4/5], Loss: 0.2108\n",
      "2024-09-26 15:54:54,172 - INFO - Epoch [5/5], Loss: 0.2085\n",
      "2024-09-26 15:54:54,263 - INFO - Query Round 7/50\n",
      "2024-09-26 15:54:56,478 - INFO - Epoch [1/5], Loss: 0.2028\n",
      "2024-09-26 15:54:58,739 - INFO - Epoch [2/5], Loss: 0.2002\n",
      "2024-09-26 15:55:00,997 - INFO - Epoch [3/5], Loss: 0.1949\n",
      "2024-09-26 15:55:03,292 - INFO - Epoch [4/5], Loss: 0.1955\n",
      "2024-09-26 15:55:05,575 - INFO - Epoch [5/5], Loss: 0.1928\n",
      "2024-09-26 15:55:15,575 - INFO - Epoch [1/5], Loss: 0.1973\n",
      "2024-09-26 15:55:21,064 - INFO - Epoch [2/5], Loss: 0.1944\n",
      "2024-09-26 15:55:26,339 - INFO - Epoch [3/5], Loss: 0.1858\n",
      "2024-09-26 15:55:31,907 - INFO - Epoch [4/5], Loss: 0.1902\n",
      "2024-09-26 15:55:38,335 - INFO - Epoch [5/5], Loss: 0.1864\n",
      "2024-09-26 15:55:38,413 - INFO - Query Round 8/50\n",
      "2024-09-26 15:55:40,846 - INFO - Epoch [1/5], Loss: 0.1807\n",
      "2024-09-26 15:55:43,888 - INFO - Epoch [2/5], Loss: 0.1802\n",
      "2024-09-26 15:55:46,503 - INFO - Epoch [3/5], Loss: 0.1809\n",
      "2024-09-26 15:55:49,101 - INFO - Epoch [4/5], Loss: 0.1729\n",
      "2024-09-26 15:55:51,489 - INFO - Epoch [5/5], Loss: 0.1730\n",
      "2024-09-26 15:56:04,064 - INFO - Epoch [1/5], Loss: 0.1813\n",
      "2024-09-26 15:56:10,652 - INFO - Epoch [2/5], Loss: 0.1740\n",
      "2024-09-26 15:56:16,450 - INFO - Epoch [3/5], Loss: 0.1719\n",
      "2024-09-26 15:56:24,408 - INFO - Epoch [4/5], Loss: 0.1714\n",
      "2024-09-26 15:56:31,569 - INFO - Epoch [5/5], Loss: 0.1644\n",
      "2024-09-26 15:56:31,645 - INFO - Query Round 9/50\n",
      "2024-09-26 15:56:34,152 - INFO - Epoch [1/5], Loss: 0.1658\n",
      "2024-09-26 15:56:36,668 - INFO - Epoch [2/5], Loss: 0.1657\n",
      "2024-09-26 15:56:39,350 - INFO - Epoch [3/5], Loss: 0.1573\n",
      "2024-09-26 15:56:42,144 - INFO - Epoch [4/5], Loss: 0.1607\n",
      "2024-09-26 15:56:44,823 - INFO - Epoch [5/5], Loss: 0.1615\n",
      "2024-09-26 15:56:59,223 - INFO - Epoch [1/5], Loss: 0.1641\n",
      "2024-09-26 15:57:05,511 - INFO - Epoch [2/5], Loss: 0.1664\n",
      "2024-09-26 15:57:11,099 - INFO - Epoch [3/5], Loss: 0.1600\n",
      "2024-09-26 15:57:16,560 - INFO - Epoch [4/5], Loss: 0.1578\n",
      "2024-09-26 15:57:22,429 - INFO - Epoch [5/5], Loss: 0.1558\n",
      "2024-09-26 15:57:22,487 - INFO - Query Round 10/50\n",
      "2024-09-26 15:57:24,856 - INFO - Epoch [1/5], Loss: 0.1525\n",
      "2024-09-26 15:57:27,222 - INFO - Epoch [2/5], Loss: 0.1525\n",
      "2024-09-26 15:57:29,803 - INFO - Epoch [3/5], Loss: 0.1503\n",
      "2024-09-26 15:57:32,686 - INFO - Epoch [4/5], Loss: 0.1473\n",
      "2024-09-26 15:57:35,102 - INFO - Epoch [5/5], Loss: 0.1472\n",
      "2024-09-26 15:57:47,168 - INFO - Epoch [1/5], Loss: 0.1547\n",
      "2024-09-26 15:57:53,706 - INFO - Epoch [2/5], Loss: 0.1528\n",
      "2024-09-26 15:58:00,079 - INFO - Epoch [3/5], Loss: 0.1475\n",
      "2024-09-26 15:58:06,327 - INFO - Epoch [4/5], Loss: 0.1489\n",
      "2024-09-26 15:58:12,826 - INFO - Epoch [5/5], Loss: 0.1459\n",
      "2024-09-26 15:58:12,890 - INFO - Iteration 10/50: Accuracy = 0.7785, F1 = 0.7015, AUC-ROC = 0.8968\n",
      "2024-09-26 15:58:12,893 - INFO - Query Round 11/50\n",
      "2024-09-26 15:58:15,695 - INFO - Epoch [1/5], Loss: 0.1458\n",
      "2024-09-26 15:58:18,243 - INFO - Epoch [2/5], Loss: 0.1404\n",
      "2024-09-26 15:58:21,075 - INFO - Epoch [3/5], Loss: 0.1412\n",
      "2024-09-26 15:58:23,660 - INFO - Epoch [4/5], Loss: 0.1408\n",
      "2024-09-26 15:58:26,191 - INFO - Epoch [5/5], Loss: 0.1357\n",
      "2024-09-26 15:58:37,644 - INFO - Epoch [1/5], Loss: 0.1452\n",
      "2024-09-26 15:58:44,195 - INFO - Epoch [2/5], Loss: 0.1439\n",
      "2024-09-26 15:58:50,541 - INFO - Epoch [3/5], Loss: 0.1380\n",
      "2024-09-26 15:58:56,616 - INFO - Epoch [4/5], Loss: 0.1372\n",
      "2024-09-26 15:59:03,197 - INFO - Epoch [5/5], Loss: 0.1402\n",
      "2024-09-26 15:59:03,259 - INFO - Query Round 12/50\n",
      "2024-09-26 15:59:05,702 - INFO - Epoch [1/5], Loss: 0.1325\n",
      "2024-09-26 15:59:08,052 - INFO - Epoch [2/5], Loss: 0.1321\n",
      "2024-09-26 15:59:10,344 - INFO - Epoch [3/5], Loss: 0.1294\n",
      "2024-09-26 15:59:12,711 - INFO - Epoch [4/5], Loss: 0.1314\n",
      "2024-09-26 15:59:15,219 - INFO - Epoch [5/5], Loss: 0.1318\n",
      "2024-09-26 15:59:26,693 - INFO - Epoch [1/5], Loss: 0.1375\n",
      "2024-09-26 15:59:32,431 - INFO - Epoch [2/5], Loss: 0.1328\n",
      "2024-09-26 15:59:38,251 - INFO - Epoch [3/5], Loss: 0.1309\n",
      "2024-09-26 15:59:44,573 - INFO - Epoch [4/5], Loss: 0.1341\n",
      "2024-09-26 15:59:52,403 - INFO - Epoch [5/5], Loss: 0.1300\n",
      "2024-09-26 15:59:52,492 - INFO - Query Round 13/50\n",
      "2024-09-26 15:59:55,715 - INFO - Epoch [1/5], Loss: 0.1300\n",
      "2024-09-26 15:59:58,555 - INFO - Epoch [2/5], Loss: 0.1269\n",
      "2024-09-26 16:00:01,012 - INFO - Epoch [3/5], Loss: 0.1267\n",
      "2024-09-26 16:00:03,428 - INFO - Epoch [4/5], Loss: 0.1222\n",
      "2024-09-26 16:00:05,817 - INFO - Epoch [5/5], Loss: 0.1209\n",
      "2024-09-26 16:00:18,889 - INFO - Epoch [1/5], Loss: 0.1270\n",
      "2024-09-26 16:00:25,417 - INFO - Epoch [2/5], Loss: 0.1269\n",
      "2024-09-26 16:00:31,492 - INFO - Epoch [3/5], Loss: 0.1291\n",
      "2024-09-26 16:00:37,578 - INFO - Epoch [4/5], Loss: 0.1236\n",
      "2024-09-26 16:00:44,328 - INFO - Epoch [5/5], Loss: 0.1211\n",
      "2024-09-26 16:00:44,392 - INFO - Query Round 14/50\n",
      "2024-09-26 16:00:46,911 - INFO - Epoch [1/5], Loss: 0.1195\n",
      "2024-09-26 16:00:49,495 - INFO - Epoch [2/5], Loss: 0.1198\n",
      "2024-09-26 16:00:51,993 - INFO - Epoch [3/5], Loss: 0.1185\n",
      "2024-09-26 16:00:54,483 - INFO - Epoch [4/5], Loss: 0.1187\n",
      "2024-09-26 16:00:56,952 - INFO - Epoch [5/5], Loss: 0.1190\n",
      "2024-09-26 16:01:08,299 - INFO - Epoch [1/5], Loss: 0.1283\n",
      "2024-09-26 16:01:14,891 - INFO - Epoch [2/5], Loss: 0.1192\n",
      "2024-09-26 16:01:22,530 - INFO - Epoch [3/5], Loss: 0.1198\n",
      "2024-09-26 16:01:29,073 - INFO - Epoch [4/5], Loss: 0.1195\n",
      "2024-09-26 16:01:35,492 - INFO - Epoch [5/5], Loss: 0.1158\n",
      "2024-09-26 16:01:35,557 - INFO - Query Round 15/50\n",
      "2024-09-26 16:01:38,034 - INFO - Epoch [1/5], Loss: 0.1164\n",
      "2024-09-26 16:01:40,493 - INFO - Epoch [2/5], Loss: 0.1145\n",
      "2024-09-26 16:01:43,153 - INFO - Epoch [3/5], Loss: 0.1133\n",
      "2024-09-26 16:01:45,694 - INFO - Epoch [4/5], Loss: 0.1169\n",
      "2024-09-26 16:01:48,170 - INFO - Epoch [5/5], Loss: 0.1125\n",
      "2024-09-26 16:01:59,191 - INFO - Epoch [1/5], Loss: 0.1268\n",
      "2024-09-26 16:02:05,091 - INFO - Epoch [2/5], Loss: 0.1182\n",
      "2024-09-26 16:02:11,147 - INFO - Epoch [3/5], Loss: 0.1159\n",
      "2024-09-26 16:02:17,328 - INFO - Epoch [4/5], Loss: 0.1191\n",
      "2024-09-26 16:02:23,486 - INFO - Epoch [5/5], Loss: 0.1131\n",
      "2024-09-26 16:02:23,545 - INFO - Iteration 15/50: Accuracy = 0.7779, F1 = 0.7075, AUC-ROC = 0.8969\n",
      "2024-09-26 16:02:23,548 - INFO - Query Round 16/50\n",
      "2024-09-26 16:02:26,078 - INFO - Epoch [1/5], Loss: 0.1098\n",
      "2024-09-26 16:02:28,554 - INFO - Epoch [2/5], Loss: 0.1094\n",
      "2024-09-26 16:02:31,108 - INFO - Epoch [3/5], Loss: 0.1101\n",
      "2024-09-26 16:02:33,600 - INFO - Epoch [4/5], Loss: 0.1069\n",
      "2024-09-26 16:02:36,089 - INFO - Epoch [5/5], Loss: 0.1132\n",
      "2024-09-26 16:02:48,723 - INFO - Epoch [1/5], Loss: 0.1198\n",
      "2024-09-26 16:02:54,862 - INFO - Epoch [2/5], Loss: 0.1176\n",
      "2024-09-26 16:03:01,192 - INFO - Epoch [3/5], Loss: 0.1095\n",
      "2024-09-26 16:03:07,433 - INFO - Epoch [4/5], Loss: 0.1136\n",
      "2024-09-26 16:03:13,839 - INFO - Epoch [5/5], Loss: 0.1108\n",
      "2024-09-26 16:03:13,918 - INFO - Query Round 17/50\n",
      "2024-09-26 16:03:16,595 - INFO - Epoch [1/5], Loss: 0.1088\n",
      "2024-09-26 16:03:19,164 - INFO - Epoch [2/5], Loss: 0.1100\n",
      "2024-09-26 16:03:21,925 - INFO - Epoch [3/5], Loss: 0.1098\n",
      "2024-09-26 16:03:24,472 - INFO - Epoch [4/5], Loss: 0.1087\n",
      "2024-09-26 16:03:27,149 - INFO - Epoch [5/5], Loss: 0.1049\n",
      "2024-09-26 16:03:39,459 - INFO - Epoch [1/5], Loss: 0.1154\n",
      "2024-09-26 16:03:46,310 - INFO - Epoch [2/5], Loss: 0.1143\n",
      "2024-09-26 16:03:53,185 - INFO - Epoch [3/5], Loss: 0.1095\n",
      "2024-09-26 16:04:00,254 - INFO - Epoch [4/5], Loss: 0.1101\n",
      "2024-09-26 16:04:07,176 - INFO - Epoch [5/5], Loss: 0.1085\n",
      "2024-09-26 16:04:07,241 - INFO - Query Round 18/50\n",
      "2024-09-26 16:04:10,004 - INFO - Epoch [1/5], Loss: 0.1047\n",
      "2024-09-26 16:04:12,680 - INFO - Epoch [2/5], Loss: 0.1077\n",
      "2024-09-26 16:04:15,466 - INFO - Epoch [3/5], Loss: 0.1020\n",
      "2024-09-26 16:04:18,282 - INFO - Epoch [4/5], Loss: 0.1031\n",
      "2024-09-26 16:04:21,144 - INFO - Epoch [5/5], Loss: 0.1022\n",
      "2024-09-26 16:04:33,812 - INFO - Epoch [1/5], Loss: 0.1096\n",
      "2024-09-26 16:04:40,738 - INFO - Epoch [2/5], Loss: 0.1090\n",
      "2024-09-26 16:04:47,386 - INFO - Epoch [3/5], Loss: 0.1089\n",
      "2024-09-26 16:04:54,439 - INFO - Epoch [4/5], Loss: 0.1034\n",
      "2024-09-26 16:05:01,460 - INFO - Epoch [5/5], Loss: 0.1058\n",
      "2024-09-26 16:05:01,537 - INFO - Query Round 19/50\n",
      "2024-09-26 16:05:04,248 - INFO - Epoch [1/5], Loss: 0.1035\n",
      "2024-09-26 16:05:07,025 - INFO - Epoch [2/5], Loss: 0.1050\n",
      "2024-09-26 16:05:09,961 - INFO - Epoch [3/5], Loss: 0.0986\n",
      "2024-09-26 16:05:12,687 - INFO - Epoch [4/5], Loss: 0.1037\n",
      "2024-09-26 16:05:15,421 - INFO - Epoch [5/5], Loss: 0.1000\n",
      "2024-09-26 16:05:28,459 - INFO - Epoch [1/5], Loss: 0.1103\n",
      "2024-09-26 16:05:34,777 - INFO - Epoch [2/5], Loss: 0.1073\n",
      "2024-09-26 16:05:41,610 - INFO - Epoch [3/5], Loss: 0.1009\n",
      "2024-09-26 16:05:48,396 - INFO - Epoch [4/5], Loss: 0.1003\n",
      "2024-09-26 16:05:55,529 - INFO - Epoch [5/5], Loss: 0.1014\n",
      "2024-09-26 16:05:55,601 - INFO - Query Round 20/50\n",
      "2024-09-26 16:05:58,281 - INFO - Epoch [1/5], Loss: 0.1028\n",
      "2024-09-26 16:06:00,985 - INFO - Epoch [2/5], Loss: 0.1027\n",
      "2024-09-26 16:06:03,682 - INFO - Epoch [3/5], Loss: 0.0963\n",
      "2024-09-26 16:06:06,359 - INFO - Epoch [4/5], Loss: 0.0933\n",
      "2024-09-26 16:06:08,927 - INFO - Epoch [5/5], Loss: 0.0985\n",
      "2024-09-26 16:06:20,996 - INFO - Epoch [1/5], Loss: 0.1107\n",
      "2024-09-26 16:06:28,233 - INFO - Epoch [2/5], Loss: 0.1048\n",
      "2024-09-26 16:06:35,130 - INFO - Epoch [3/5], Loss: 0.1021\n",
      "2024-09-26 16:06:41,850 - INFO - Epoch [4/5], Loss: 0.0994\n",
      "2024-09-26 16:06:48,172 - INFO - Epoch [5/5], Loss: 0.0981\n",
      "2024-09-26 16:06:48,227 - INFO - Iteration 20/50: Accuracy = 0.7809, F1 = 0.7097, AUC-ROC = 0.8961\n",
      "2024-09-26 16:06:48,230 - INFO - Query Round 21/50\n",
      "2024-09-26 16:06:50,686 - INFO - Epoch [1/5], Loss: 0.0958\n",
      "2024-09-26 16:06:53,671 - INFO - Epoch [2/5], Loss: 0.0960\n",
      "2024-09-26 16:06:56,488 - INFO - Epoch [3/5], Loss: 0.0980\n",
      "2024-09-26 16:06:59,199 - INFO - Epoch [4/5], Loss: 0.0970\n",
      "2024-09-26 16:07:01,989 - INFO - Epoch [5/5], Loss: 0.0954\n",
      "2024-09-26 16:07:16,368 - INFO - Epoch [1/5], Loss: 0.1063\n",
      "2024-09-26 16:07:23,562 - INFO - Epoch [2/5], Loss: 0.1014\n",
      "2024-09-26 16:07:31,157 - INFO - Epoch [3/5], Loss: 0.0997\n",
      "2024-09-26 16:07:38,439 - INFO - Epoch [4/5], Loss: 0.0932\n",
      "2024-09-26 16:07:45,678 - INFO - Epoch [5/5], Loss: 0.0975\n",
      "2024-09-26 16:07:45,761 - INFO - Query Round 22/50\n",
      "2024-09-26 16:07:48,489 - INFO - Epoch [1/5], Loss: 0.0959\n",
      "2024-09-26 16:07:51,325 - INFO - Epoch [2/5], Loss: 0.0940\n",
      "2024-09-26 16:07:54,193 - INFO - Epoch [3/5], Loss: 0.0944\n",
      "2024-09-26 16:07:57,371 - INFO - Epoch [4/5], Loss: 0.0940\n",
      "2024-09-26 16:08:00,242 - INFO - Epoch [5/5], Loss: 0.0945\n",
      "2024-09-26 16:08:12,467 - INFO - Epoch [1/5], Loss: 0.1018\n",
      "2024-09-26 16:08:19,962 - INFO - Epoch [2/5], Loss: 0.1019\n",
      "2024-09-26 16:08:27,145 - INFO - Epoch [3/5], Loss: 0.0973\n",
      "2024-09-26 16:08:34,140 - INFO - Epoch [4/5], Loss: 0.0967\n",
      "2024-09-26 16:08:41,578 - INFO - Epoch [5/5], Loss: 0.0926\n",
      "2024-09-26 16:08:41,645 - INFO - Early stopping at iteration 22 due to no improvement\n",
      "2024-09-26 16:08:41,650 - INFO - Active learning completed in 1093.51 seconds\n",
      "2024-09-26 16:08:41,651 - INFO - Final performance: Accuracy = 0.7803, F1 = 0.7102, AUC-ROC = 0.8970\n",
      "2024-09-26 16:08:41,737 - INFO - PyTorch Active Learning Pipeline completed in 1094.89 seconds\n",
      "2024-09-26 16:08:41,738 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " antiscience       0.71      0.73      0.72      3736\n",
      "     neutral       0.84      0.88      0.86      6678\n",
      "  proscience       0.66      0.48      0.56      1506\n",
      "\n",
      "    accuracy                           0.78     11920\n",
      "   macro avg       0.74      0.69      0.71     11920\n",
      "weighted avg       0.78      0.78      0.78     11920\n",
      "\n",
      "2024-09-26 16:08:43,802 - INFO - Confusion matrix saved as 'pytorch_confusion_matrix.png'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " antiscience       0.71      0.73      0.72      3736\n",
      "     neutral       0.84      0.88      0.86      6678\n",
      "  proscience       0.66      0.48      0.56      1506\n",
      "\n",
      "    accuracy                           0.78     11920\n",
      "   macro avg       0.74      0.69      0.71     11920\n",
      "weighted avg       0.78      0.78      0.78     11920\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2713  805  218]\n",
      " [ 663 5869  146]\n",
      " [ 449  338  719]]\n",
      "\n",
      "Accuracy history saved as 'nn_accuracy_history.png'\n",
      "AUC-ROC history saved as 'nn_auc_roc_history.png'\n",
      "\n",
      "Final Model Performance:\n",
      "Accuracy: 0.7803\n",
      "F1 Score: 0.7102\n",
      "AUC-ROC: 0.8970\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PyTorchActiveLearningPipeline:\n",
    "    def __init__(self, labeled_data_file: str, augmented_data_file: str, unlabeled_data_file: str, \n",
    "                 embedding_column: str, label_column: str, test_size: float = 0.2, \n",
    "                 initial_labeled_ratio: float = 0.3, random_state: int = 42, batch_size: int = 64):\n",
    "        self.labeled_data_file = labeled_data_file\n",
    "        self.augmented_data_file = augmented_data_file\n",
    "        self.unlabeled_data_file = unlabeled_data_file\n",
    "        self.embedding_column = embedding_column\n",
    "        self.label_column = label_column\n",
    "        self.test_size = test_size\n",
    "        self.initial_labeled_ratio = initial_labeled_ratio\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "        self._log_init_info()\n",
    "        self._load_and_preprocess_data()\n",
    "        \n",
    "    def _log_init_info(self) -> None:\n",
    "        logger.info(\"Initializing PyTorchActiveLearningPipeline\")\n",
    "        logger.info(f\"Labeled data file: {self.labeled_data_file}\")\n",
    "        logger.info(f\"Augmented data file: {self.augmented_data_file}\")\n",
    "        logger.info(f\"Unlabeled data file: {self.unlabeled_data_file}\")\n",
    "        logger.info(f\"Embedding column: {self.embedding_column}\")\n",
    "        logger.info(f\"Label column: {self.label_column}\")\n",
    "        logger.info(f\"Test size: {self.test_size}\")\n",
    "        logger.info(f\"Initial labeled ratio: {self.initial_labeled_ratio}\")\n",
    "        logger.info(f\"Random state: {self.random_state}\")\n",
    "        logger.info(f\"Batch size: {self.batch_size}\")\n",
    "\n",
    "    def _process_embeddings(self, df):\n",
    "        def process_embedding(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip('[]')\n",
    "                try:\n",
    "                    return np.array([float(i) for i in x.split(',')])\n",
    "                except ValueError:\n",
    "                    return np.array([float(i) for i in x.split()])\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                return x\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected embedding format: {type(x)}\")\n",
    "\n",
    "        df[self.embedding_column] = df[self.embedding_column].apply(process_embedding)\n",
    "        return df\n",
    "\n",
    "    def _load_and_preprocess_data(self) -> None:\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        self.labeled_data = pd.read_csv(self.labeled_data_file)\n",
    "        self.labeled_data = self._process_embeddings(self.labeled_data)\n",
    "        logger.info(f\"Labeled data shape: {self.labeled_data.shape}\")\n",
    "        \n",
    "        self.augmented_data = pd.read_csv(self.augmented_data_file)\n",
    "        self.augmented_data = self._process_embeddings(self.augmented_data)\n",
    "        logger.info(f\"Augmented data shape: {self.augmented_data.shape}\")\n",
    "        \n",
    "        all_data = pd.concat([self.labeled_data, self.augmented_data])\n",
    "        \n",
    "        self.le.fit(all_data[self.label_column])\n",
    "        self.labeled_data['encoded_label'] = self.le.transform(self.labeled_data[self.label_column])\n",
    "        self.augmented_data['encoded_label'] = self.le.transform(self.augmented_data[self.label_column])\n",
    "        \n",
    "        # Load unlabeled data\n",
    "        self.dimension = len(self.labeled_data[self.embedding_column].iloc[0])\n",
    "        self.unlabeled_data = []\n",
    "        parquet_file = pq.ParquetFile(self.unlabeled_data_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.batch_size, columns=[self.embedding_column]):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            df_chunk = self._process_embeddings(df_chunk)\n",
    "            embeddings = np.stack(df_chunk[self.embedding_column].values)\n",
    "            self.unlabeled_data.append(embeddings)\n",
    "        \n",
    "        self.unlabeled_data = np.vstack(self.unlabeled_data)\n",
    "        logger.info(f\"Unlabeled data shape: {self.unlabeled_data.shape}\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(self.unlabeled_data.astype('float32'))\n",
    "        logger.info(f\"FAISS index size: {self.index.ntotal}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Data loaded and preprocessed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        logger.info(\"Preparing data for active learning\")\n",
    "        \n",
    "        X = np.vstack([\n",
    "            np.stack(self.labeled_data[self.embedding_column].values),\n",
    "            np.stack(self.augmented_data[self.embedding_column].values)\n",
    "        ])\n",
    "        y = np.concatenate([\n",
    "            self.labeled_data['encoded_label'].values,\n",
    "            self.augmented_data['encoded_label'].values\n",
    "        ])\n",
    "        \n",
    "        # Ensure that the initial labeled set contains samples from all classes\n",
    "        classes = np.unique(y)\n",
    "        initial_indices = []\n",
    "        for cls in classes:\n",
    "            idx = np.where(y == cls)[0]\n",
    "            initial_indices.append(idx[0])  # Take the first instance of each class\n",
    "\n",
    "        remaining_indices = [i for i in range(len(y)) if i not in initial_indices]\n",
    "\n",
    "        X_initial = X[initial_indices]\n",
    "        y_initial = y[initial_indices]\n",
    "\n",
    "        # Now sample the rest of the initial data\n",
    "        additional_initial_size = int(self.initial_labeled_ratio * len(X)) - len(initial_indices)\n",
    "        X_remaining = X[remaining_indices]\n",
    "        y_remaining = y[remaining_indices]\n",
    "\n",
    "        X_additional_initial, X_pool, y_additional_initial, y_pool = train_test_split(\n",
    "            X_remaining, y_remaining, train_size=additional_initial_size,\n",
    "            random_state=self.random_state, stratify=y_remaining\n",
    "        )\n",
    "\n",
    "        X_initial = np.vstack([X_initial, X_additional_initial])\n",
    "        y_initial = np.concatenate([y_initial, y_additional_initial])\n",
    "\n",
    "        # Split the pool into pool and test sets\n",
    "        X_pool, X_test, y_pool, y_test = train_test_split(\n",
    "            X_pool, y_pool, test_size=self.test_size, random_state=self.random_state, stratify=y_pool\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Initial labeled set shape: {X_initial.shape}\")\n",
    "        logger.info(f\"Pool set shape: {X_pool.shape}\")\n",
    "        logger.info(f\"Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        self.X_initial = torch.tensor(X_initial, dtype=torch.float32)\n",
    "        self.y_initial = torch.tensor(y_initial, dtype=torch.long)\n",
    "        self.X_pool = torch.tensor(X_pool, dtype=torch.float32)\n",
    "        self.y_pool = torch.tensor(y_pool, dtype=torch.long)  # For simulation\n",
    "        self.X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        self.y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    def train_and_evaluate(self, model, X_test, y_test):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred_proba = F.softmax(outputs, dim=1)\n",
    "\n",
    "            y_test_np = y_test.numpy()\n",
    "            y_pred_np = y_pred.numpy()\n",
    "            y_pred_proba_np = y_pred_proba.numpy()\n",
    "\n",
    "            accuracy = accuracy_score(y_test_np, y_pred_np)\n",
    "            f1 = f1_score(y_test_np, y_pred_np, average='macro')\n",
    "            auc_roc = roc_auc_score(y_test_np, y_pred_proba_np, multi_class='ovr')\n",
    "\n",
    "        return accuracy, f1, auc_roc\n",
    "\n",
    "    def get_uncertainty_scores(self, model, X_pool):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_pool)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-5), dim=1)\n",
    "        return entropy\n",
    "\n",
    "    def query_samples(self, model, X_pool, n_instances):\n",
    "        uncertainties = self.get_uncertainty_scores(model, X_pool)\n",
    "        _, query_indices = torch.topk(uncertainties, n_instances)\n",
    "        return query_indices\n",
    "\n",
    "    def find_similar_samples(self, sample, k=5):\n",
    "        sample_np = sample.numpy().reshape(1, -1).astype('float32')\n",
    "        distances, indices = self.index.search(sample_np, k)\n",
    "        return indices[0]  # Return indices of similar samples\n",
    "\n",
    "    def train_with_active_learning(self, n_queries=50, n_instances_per_query=100, k_similar=5):\n",
    "        logger.info(\"Starting active learning with PyTorch and consistency regularization\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        input_dim = self.X_initial.shape[1]\n",
    "        num_classes = len(self.le.classes_)\n",
    "        model = ClassificationModel(input_dim, num_classes)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        num_epochs = 5\n",
    "        lambda_c = 1.0  # Weight for consistency loss\n",
    "\n",
    "        # Initialize labeled dataset and dataloader\n",
    "        X_labeled = self.X_initial.clone()\n",
    "        y_labeled = self.y_initial.clone()\n",
    "\n",
    "        labeled_dataset = LabeledDataset(X_labeled, y_labeled)\n",
    "        labeled_loader = DataLoader(labeled_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        X_pool = self.X_pool.clone()\n",
    "        y_pool = self.y_pool.clone()\n",
    "        X_unlabeled = torch.tensor(self.unlabeled_data, dtype=torch.float32)\n",
    "\n",
    "        performance_history = []\n",
    "        best_accuracy = 0\n",
    "        iterations_without_improvement = 0\n",
    "        max_iterations_without_improvement = 10\n",
    "\n",
    "        for query_round in tqdm(range(n_queries), desc=\"Active Learning Progress\"):\n",
    "            logger.info(f\"Query Round {query_round + 1}/{n_queries}\")\n",
    "\n",
    "            # Training the model\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "\n",
    "                for X_batch, y_batch in labeled_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_batch)\n",
    "                    supervised_loss = F.cross_entropy(outputs, y_batch)\n",
    "\n",
    "                    # No consistency loss in initial training\n",
    "                    consistency_loss = 0.0\n",
    "\n",
    "                    loss = supervised_loss + lambda_c * consistency_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(labeled_loader):.4f}\")\n",
    "\n",
    "            # Query new samples\n",
    "            query_indices = self.query_samples(model, X_pool, n_instances_per_query)\n",
    "            X_query = X_pool[query_indices]\n",
    "            y_query = y_pool[query_indices]  # For simulation; in practice, get labels from oracle\n",
    "\n",
    "            # Find similar samples using FAISS\n",
    "            similar_indices_list = []\n",
    "            for sample in X_query:\n",
    "                indices = self.find_similar_samples(sample, k=k_similar)\n",
    "                similar_indices_list.extend(indices)\n",
    "\n",
    "            similar_indices = list(set(similar_indices_list))\n",
    "            X_similar = X_unlabeled[similar_indices]\n",
    "\n",
    "            # Remove similar samples from X_unlabeled and update FAISS index\n",
    "            mask_unlabeled = torch.ones(len(X_unlabeled), dtype=torch.bool)\n",
    "            mask_unlabeled[similar_indices] = False\n",
    "            X_unlabeled = X_unlabeled[mask_unlabeled]\n",
    "\n",
    "            # Rebuild FAISS index\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "            self.index.add(X_unlabeled.numpy().astype('float32'))\n",
    "\n",
    "            # Update labeled dataset\n",
    "            X_labeled = torch.cat([X_labeled, X_query], dim=0)\n",
    "            y_labeled = torch.cat([y_labeled, y_query], dim=0)\n",
    "\n",
    "            # Remove queried samples from X_pool and y_pool\n",
    "            mask_pool = torch.ones(len(X_pool), dtype=torch.bool)\n",
    "            mask_pool[query_indices] = False\n",
    "            X_pool = X_pool[mask_pool]\n",
    "            y_pool = y_pool[mask_pool]\n",
    "\n",
    "            # Update the labeled dataset and dataloader\n",
    "            labeled_dataset = LabeledDataset(X_labeled, y_labeled)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            # Retrain the model with consistency regularization\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "\n",
    "                for X_batch, y_batch in labeled_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_batch)\n",
    "                    supervised_loss = F.cross_entropy(outputs, y_batch)\n",
    "\n",
    "                    # Consistency loss between queried samples and similar samples\n",
    "                    consistency_loss = 0.0\n",
    "                    if X_query.size(0) > 0 and X_similar.size(0) > 0:\n",
    "                        outputs_query = model(X_query)\n",
    "                        probs_query = F.softmax(outputs_query, dim=1)\n",
    "\n",
    "                        outputs_similar = model(X_similar)\n",
    "                        probs_similar = F.softmax(outputs_similar, dim=1)\n",
    "\n",
    "                        # For each queried sample, compute consistency loss with its similar samples\n",
    "                        consistency_loss = F.mse_loss(probs_query.mean(dim=0), probs_similar.mean(dim=0))\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = supervised_loss + lambda_c * consistency_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(labeled_loader):.4f}\")\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy, f1, auc_roc = self.train_and_evaluate(model, self.X_test, self.y_test)\n",
    "            performance_history.append((accuracy, f1, auc_roc))\n",
    "\n",
    "            # Early stopping logic\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                iterations_without_improvement = 0\n",
    "                best_model = model\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "\n",
    "            if (query_round + 1) % 5 == 0:\n",
    "                logger.info(f\"Iteration {query_round + 1}/{n_queries}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "            if iterations_without_improvement >= max_iterations_without_improvement:\n",
    "                logger.info(f\"Early stopping at iteration {query_round + 1} due to no improvement\")\n",
    "                break\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Active learning completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Final performance: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "        return performance_history, best_model\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        logger.info(\"Starting PyTorch active learning pipeline\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.prepare_data()\n",
    "        performance_history, final_model = self.train_with_active_learning()\n",
    "\n",
    "        # Evaluate final model\n",
    "        final_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = final_model(self.X_test)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred_np = y_pred.numpy()\n",
    "            y_test_np = self.y_test.numpy()\n",
    "\n",
    "            cm = confusion_matrix(y_test_np, y_pred_np)\n",
    "            class_report = classification_report(y_test_np, y_pred_np, target_names=self.le.classes_)\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"PyTorch Active Learning Pipeline completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(\"Classification Report:\\n\" + class_report)\n",
    "\n",
    "        self.plot_performance_history(performance_history)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "\n",
    "        return performance_history, cm, class_report, final_model\n",
    "\n",
    "    def plot_performance_history(self, performance_history):\n",
    "        queries = range(1, len(performance_history) + 1)\n",
    "        accuracies, f1_scores, auc_rocs = zip(*performance_history)\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(queries, accuracies, label='Accuracy', marker='o')\n",
    "        plt.xlabel('Iteration Number')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Performance (Accuracy) Over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('accuracy_history.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot F1 Score\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(queries, f1_scores, label='F1 Score', marker='o')\n",
    "        plt.xlabel('Iteration Number')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Performance (F1 Score) Over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('f1_score_history.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot AUC-ROC\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(queries, auc_rocs, label='AUC-ROC', marker='o')\n",
    "        plt.xlabel('Iteration Number')\n",
    "        plt.ylabel('AUC-ROC Score')\n",
    "        plt.title('Performance (AUC-ROC) Over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('auc_roc_history.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
    "        plt.title('FAISS and Consistency Regularization Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pytorch_confusion_matrix.png')\n",
    "        logger.info(\"Confusion matrix saved as 'pytorch_confusion_matrix.png'\")\n",
    "        plt.close()\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Usage\n",
    "labeled_data_file = 'sbert_data_with_distance_features.csv'\n",
    "augmented_data_file = 'cleaned_proscience_da.csv'\n",
    "unlabeled_data_file = r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet'\n",
    "\n",
    "active_learning_pipeline = PyTorchActiveLearningPipeline(\n",
    "    labeled_data_file=labeled_data_file,\n",
    "    augmented_data_file=augmented_data_file,\n",
    "    unlabeled_data_file=unlabeled_data_file,\n",
    "    embedding_column=\"dense_embedding\",\n",
    "    label_column=\"cleaned_classification\"\n",
    ")\n",
    "\n",
    "performance_history, cm, class_report, final_model = active_learning_pipeline.run_pipeline()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nAccuracy history saved as 'nn_accuracy_history.png'\")\n",
    "print(\"AUC-ROC history saved as 'nn_auc_roc_history.png'\")\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"Accuracy: {performance_history[-1][0]:.4f}\")\n",
    "print(f\"F1 Score: {performance_history[-1][1]:.4f}\")\n",
    "print(f\"AUC-ROC: {performance_history[-1][2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NN with one sided kl divergence and focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 01:44:48,981 - INFO - Loading and preprocessing data...\n",
      "2024-09-26 01:45:08,738 - INFO - Labeled data shape: (79763, 13)\n",
      "2024-09-26 01:45:08,739 - INFO - Augmented data shape: (5378, 5)\n",
      "2024-09-26 01:45:08,740 - INFO - Unlabeled data shape: (412879, 384)\n",
      "2024-09-26 01:45:08,741 - INFO - FAISS index size: 412879\n",
      "2024-09-26 01:45:08,747 - INFO - Preparing data for active learning\n",
      "2024-09-26 01:45:09,442 - INFO - Starting advanced active learning pipeline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ff37268c2f4db2adac7564acb5424b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Active Learning Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 01:45:21,814 - INFO - Query 1, Accuracy: 0.7597, F1: 0.6395, AUC-ROC: 0.8806\n",
      "2024-09-26 01:47:18,405 - INFO - Query 2, Accuracy: 0.7527, F1: 0.6328, AUC-ROC: 0.8785\n",
      "2024-09-26 01:49:39,327 - INFO - Query 3, Accuracy: 0.7561, F1: 0.6624, AUC-ROC: 0.8792\n",
      "2024-09-26 01:52:15,026 - INFO - Query 4, Accuracy: 0.7554, F1: 0.6577, AUC-ROC: 0.8782\n",
      "2024-09-26 01:54:40,006 - INFO - Query 5, Accuracy: 0.7532, F1: 0.6652, AUC-ROC: 0.8768\n",
      "2024-09-26 01:57:11,211 - INFO - Query 6, Accuracy: 0.7566, F1: 0.6628, AUC-ROC: 0.8783\n",
      "2024-09-26 01:59:54,612 - INFO - Query 7, Accuracy: 0.7511, F1: 0.6673, AUC-ROC: 0.8758\n",
      "2024-09-26 02:02:38,332 - INFO - Query 8, Accuracy: 0.7497, F1: 0.6604, AUC-ROC: 0.8738\n",
      "2024-09-26 02:05:34,754 - INFO - Query 9, Accuracy: 0.7471, F1: 0.6598, AUC-ROC: 0.8735\n",
      "2024-09-26 02:08:27,075 - INFO - Query 10, Accuracy: 0.7551, F1: 0.6613, AUC-ROC: 0.8746\n",
      "2024-09-26 02:11:24,628 - INFO - Query 11, Accuracy: 0.7563, F1: 0.6638, AUC-ROC: 0.8744\n",
      "2024-09-26 02:14:24,439 - INFO - Query 12, Accuracy: 0.7571, F1: 0.6699, AUC-ROC: 0.8741\n",
      "2024-09-26 02:17:16,582 - INFO - Query 13, Accuracy: 0.7545, F1: 0.6553, AUC-ROC: 0.8737\n",
      "2024-09-26 02:20:12,882 - INFO - Query 14, Accuracy: 0.7556, F1: 0.6673, AUC-ROC: 0.8746\n",
      "2024-09-26 02:23:01,986 - INFO - Query 15, Accuracy: 0.7567, F1: 0.6613, AUC-ROC: 0.8730\n",
      "2024-09-26 02:25:45,851 - INFO - Query 16, Accuracy: 0.7559, F1: 0.6662, AUC-ROC: 0.8750\n",
      "2024-09-26 08:51:31,426 - INFO - Query 17, Accuracy: 0.7591, F1: 0.6696, AUC-ROC: 0.8756\n",
      "2024-09-26 09:42:57,576 - INFO - Query 18, Accuracy: 0.7589, F1: 0.6692, AUC-ROC: 0.8759\n",
      "2024-09-26 09:45:29,648 - INFO - Query 19, Accuracy: 0.7560, F1: 0.6649, AUC-ROC: 0.8753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 356\u001b[0m\n\u001b[0;32m    346\u001b[0m unlabeled_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnrosso\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mthesis_project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124membeddings.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    348\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m AdvancedActiveLearningPipeline(\n\u001b[0;32m    349\u001b[0m     labeled_data_file\u001b[38;5;241m=\u001b[39mlabeled_data_file,\n\u001b[0;32m    350\u001b[0m     augmented_data_file\u001b[38;5;241m=\u001b[39maugmented_data_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m     label_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m )\n\u001b[1;32m--> 356\u001b[0m performance_history, final_model, final_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_metrics[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_metrics[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 321\u001b[0m, in \u001b[0;36mAdvancedActiveLearningPipeline.run_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m--> 321\u001b[0m     performance_history, final_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_active_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     final_accuracy, final_precision, final_recall, final_f1, final_auc_roc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_model(final_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test)\n",
      "Cell \u001b[1;32mIn[3], line 236\u001b[0m, in \u001b[0;36mAdvancedActiveLearningPipeline.train_with_active_learning\u001b[1;34m(self, n_queries, n_instances_per_query, k_similar)\u001b[0m\n\u001b[0;32m    234\u001b[0m     ramp_up_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(query_round \u001b[38;5;241m/\u001b[39m (n_queries \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Linear ramp-up\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m adaptive_temperature(outputs)\n\u001b[1;32m--> 236\u001b[0m     consistency_loss \u001b[38;5;241m=\u001b[39m \u001b[43mone_sided_kl_divergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_labeled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_similar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ramp_up_factor \u001b[38;5;241m*\u001b[39m consistency_loss\n\u001b[0;32m    239\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mone_sided_kl_divergence\u001b[1;34m(model, labeled_embeddings, similar_embeddings, temperature)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_sided_kl_divergence\u001b[39m(model, labeled_embeddings, similar_embeddings, temperature):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 47\u001b[0m         labeled_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m         labeled_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(labeled_logits \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m         labeled_probs_mean \u001b[38;5;241m=\u001b[39m labeled_probs\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 81\u001b[0m, in \u001b[0;36mClassificationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MultilabelFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma=2, reduction='mean'):\n",
    "        super(MultilabelFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        alpha_t = self.alpha[targets.data.view(-1).long()].view_as(targets)\n",
    "        F_loss = alpha_t * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "def adaptive_temperature(model_output, base_temperature=1.0, min_temperature=0.5, max_temperature=5.0):\n",
    "    confidence = F.softmax(model_output, dim=1).max(1)[0].mean()\n",
    "    temperature = base_temperature / confidence\n",
    "    return torch.clamp(temperature, min_temperature, max_temperature)\n",
    "\n",
    "def one_sided_kl_divergence(model, labeled_embeddings, similar_embeddings, temperature):\n",
    "    with torch.no_grad():\n",
    "        labeled_logits = model(labeled_embeddings)\n",
    "        labeled_probs = F.softmax(labeled_logits / temperature, dim=-1)\n",
    "        labeled_probs_mean = labeled_probs.mean(dim=0)\n",
    "    \n",
    "    similar_logits = model(similar_embeddings)\n",
    "    similar_probs = F.softmax(similar_logits / temperature, dim=-1)\n",
    "    \n",
    "    # Expand labeled_probs_mean to match the batch size of similar_probs\n",
    "    labeled_probs_mean_expanded = labeled_probs_mean.unsqueeze(0).expand(similar_probs.size(0), -1)\n",
    "    \n",
    "    kl_div = F.kl_div(similar_probs.log(), labeled_probs_mean_expanded, reduction='batchmean')\n",
    "    return kl_div\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class AdvancedActiveLearningPipeline:\n",
    "    def __init__(self, labeled_data_file, augmented_data_file, unlabeled_data_file,\n",
    "                 embedding_column, label_column, test_size=0.2, initial_labeled_ratio=0.3,\n",
    "                 random_state=42, batch_size=64):\n",
    "        self.labeled_data_file = labeled_data_file\n",
    "        self.augmented_data_file = augmented_data_file\n",
    "        self.unlabeled_data_file = unlabeled_data_file\n",
    "        self.embedding_column = embedding_column\n",
    "        self.label_column = label_column\n",
    "        self.test_size = test_size\n",
    "        self.initial_labeled_ratio = initial_labeled_ratio\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.le = LabelEncoder()\n",
    "\n",
    "        self._load_and_preprocess_data()\n",
    "\n",
    "    def _process_embeddings(self, df):\n",
    "        def process_embedding(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip('[]')\n",
    "                try:\n",
    "                    return np.array([float(i) for i in x.split(',')])\n",
    "                except ValueError:\n",
    "                    return np.array([float(i) for i in x.split()])\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                return x\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected embedding format: {type(x)}\")\n",
    "\n",
    "        df[self.embedding_column] = df[self.embedding_column].apply(process_embedding)\n",
    "        return df\n",
    "\n",
    "    def _load_and_preprocess_data(self):\n",
    "        logger.info(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        self.labeled_data = pd.read_csv(self.labeled_data_file)\n",
    "        self.labeled_data = self._process_embeddings(self.labeled_data)\n",
    "        \n",
    "        self.augmented_data = pd.read_csv(self.augmented_data_file)\n",
    "        self.augmented_data = self._process_embeddings(self.augmented_data)\n",
    "        \n",
    "        all_data = pd.concat([self.labeled_data, self.augmented_data])\n",
    "        \n",
    "        self.le.fit(all_data[self.label_column])\n",
    "        self.labeled_data['encoded_label'] = self.le.transform(self.labeled_data[self.label_column])\n",
    "        self.augmented_data['encoded_label'] = self.le.transform(self.augmented_data[self.label_column])\n",
    "        \n",
    "        self.dimension = len(self.labeled_data[self.embedding_column].iloc[0])\n",
    "        self.unlabeled_data = []\n",
    "        parquet_file = pq.ParquetFile(self.unlabeled_data_file)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.batch_size, columns=[self.embedding_column]):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            df_chunk = self._process_embeddings(df_chunk)\n",
    "            embeddings = np.stack(df_chunk[self.embedding_column].values)\n",
    "            self.unlabeled_data.append(embeddings)\n",
    "        \n",
    "        self.unlabeled_data = np.vstack(self.unlabeled_data)\n",
    "        \n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(self.unlabeled_data.astype('float32'))\n",
    "\n",
    "        logger.info(f\"Labeled data shape: {self.labeled_data.shape}\")\n",
    "        logger.info(f\"Augmented data shape: {self.augmented_data.shape}\")\n",
    "        logger.info(f\"Unlabeled data shape: {self.unlabeled_data.shape}\")\n",
    "        logger.info(f\"FAISS index size: {self.index.ntotal}\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        logger.info(\"Preparing data for active learning\")\n",
    "        \n",
    "        X = np.vstack([\n",
    "            np.stack(self.labeled_data[self.embedding_column].values),\n",
    "            np.stack(self.augmented_data[self.embedding_column].values)\n",
    "        ])\n",
    "        y = np.concatenate([\n",
    "            self.labeled_data['encoded_label'].values,\n",
    "            self.augmented_data['encoded_label'].values\n",
    "        ])\n",
    "        \n",
    "        classes, class_counts = np.unique(y, return_counts=True)\n",
    "        class_weights = 1 / (class_counts / np.sum(class_counts))\n",
    "        self.class_weights = class_weights / np.sum(class_weights)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        initial_size = int(self.initial_labeled_ratio * len(X_train))\n",
    "        X_initial, X_pool, y_initial, y_pool = train_test_split(\n",
    "            X_train, y_train, train_size=initial_size,\n",
    "            random_state=self.random_state, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        self.X_initial = torch.tensor(X_initial, dtype=torch.float32)\n",
    "        self.y_initial = torch.tensor(y_initial, dtype=torch.long)\n",
    "        self.X_pool = torch.tensor(X_pool, dtype=torch.float32)\n",
    "        self.y_pool = torch.tensor(y_pool, dtype=torch.long)\n",
    "        self.X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        self.y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    def evaluate_model(self, model, X, y):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        y_np = y.numpy()\n",
    "        preds_np = preds.numpy()\n",
    "        probs_np = probs.numpy()\n",
    "        \n",
    "        accuracy = accuracy_score(y_np, preds_np)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_np, preds_np, average='macro')\n",
    "        \n",
    "        # Calculate AUC-ROC\n",
    "        auc_roc = roc_auc_score(y_np, probs_np, multi_class='ovr', average='macro')\n",
    "        \n",
    "        return accuracy, precision, recall, f1, auc_roc\n",
    "\n",
    "    def train_with_active_learning(self, n_queries=50, n_instances_per_query=100, k_similar=5):\n",
    "        logger.info(\"Starting advanced active learning pipeline\")\n",
    "        \n",
    "        num_classes = len(self.le.classes_)\n",
    "        model = ClassificationModel(self.dimension, num_classes)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        focal_loss = MultilabelFocalLoss(alpha=self.class_weights.tolist())\n",
    "        \n",
    "        X_labeled = self.X_initial.clone()\n",
    "        y_labeled = self.y_initial.clone()\n",
    "        \n",
    "        performance_history = []\n",
    "        \n",
    "        for query_round in tqdm(range(n_queries), desc=\"Active Learning Progress\"):\n",
    "            labeled_dataset = LabeledDataset(X_labeled, y_labeled)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            \n",
    "            # Training loop\n",
    "            model.train()\n",
    "            for epoch in range(5):  # You can adjust the number of epochs\n",
    "                for X_batch, y_batch in labeled_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = focal_loss(outputs, F.one_hot(y_batch, num_classes=num_classes).float())\n",
    "                    \n",
    "                    # Consistency regularization with ramp-up schedule\n",
    "                    if query_round > 0:\n",
    "                        ramp_up_factor = min(query_round / (n_queries / 2), 1.0)  # Linear ramp-up\n",
    "                        temperature = adaptive_temperature(outputs)\n",
    "                        consistency_loss = one_sided_kl_divergence(model, X_labeled, X_similar, temperature)\n",
    "                        loss += ramp_up_factor * consistency_loss\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Query new samples\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pool_outputs = model(self.X_pool)\n",
    "                pool_probs = F.softmax(pool_outputs, dim=1)\n",
    "                uncertainties = -(pool_probs * torch.log(pool_probs + 1e-8)).sum(1)\n",
    "                query_indices = uncertainties.topk(n_instances_per_query).indices\n",
    "            \n",
    "            X_query = self.X_pool[query_indices]\n",
    "            y_query = self.y_pool[query_indices]\n",
    "            \n",
    "            # Find similar samples using FAISS\n",
    "            similar_indices = []\n",
    "            for sample in X_query:\n",
    "                _, indices = self.index.search(sample.numpy().reshape(1, -1).astype('float32'), k_similar)\n",
    "                similar_indices.extend(indices[0])\n",
    "            \n",
    "            X_similar = torch.tensor(self.unlabeled_data[similar_indices], dtype=torch.float32)\n",
    "            \n",
    "            # Update labeled dataset\n",
    "            X_labeled = torch.cat([X_labeled, X_query])\n",
    "            y_labeled = torch.cat([y_labeled, y_query])\n",
    "            \n",
    "            # Remove queried samples from pool\n",
    "            mask = torch.ones(len(self.X_pool), dtype=torch.bool)\n",
    "            mask[query_indices] = False\n",
    "            self.X_pool = self.X_pool[mask]\n",
    "            self.y_pool = self.y_pool[mask]\n",
    "            \n",
    "            #Evaluate model\n",
    "            accuracy, precision, recall, f1, auc_roc = self.evaluate_model(model, self.X_test, self.y_test)\n",
    "            performance_history.append((accuracy, precision, recall, f1, auc_roc))\n",
    "            \n",
    "            logger.info(f\"Query {query_round + 1}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}, AUC-ROC: {auc_roc:.4f}\")\n",
    "        \n",
    "        return performance_history, model\n",
    "\n",
    "    def plot_performance_history(self, performance_history):\n",
    "        queries = range(1, len(performance_history) + 1)\n",
    "        accuracies, _, _, _, auc_rocs = zip(*performance_history)\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(queries, accuracies, label='QBC Accuracy', marker='o')\n",
    "        plt.xlabel('Iteration Number')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('QBC Performance (Accuracy) Over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('qbc_accuracy_history.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot AUC-ROC\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(queries, auc_rocs, label='QBC AUC-ROC', marker='o')\n",
    "        plt.xlabel('Iteration Number')\n",
    "        plt.ylabel('AUC-ROC Score')\n",
    "        plt.title('QBC Performance (AUC-ROC) Over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('qbc_auc_roc_history.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        self.prepare_data()\n",
    "        performance_history, final_model = self.train_with_active_learning()\n",
    "        \n",
    "        # Final evaluation\n",
    "        final_accuracy, final_precision, final_recall, final_f1, final_auc_roc = self.evaluate_model(final_model, self.X_test, self.y_test)\n",
    "        \n",
    "        logger.info(f\"Final Test Accuracy: {final_accuracy:.4f}\")\n",
    "        logger.info(f\"Final Test Precision (macro): {final_precision:.4f}\")\n",
    "        logger.info(f\"Final Test Recall (macro): {final_recall:.4f}\")\n",
    "        logger.info(f\"Final Test F1 Score (macro): {final_f1:.4f}\")\n",
    "        logger.info(f\"Final Test AUC-ROC (macro): {final_auc_roc:.4f}\")\n",
    "        \n",
    "        # Plot performance history\n",
    "        self.plot_performance_history(performance_history)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        with torch.no_grad():\n",
    "            test_outputs = final_model(self.X_test)\n",
    "            _, test_preds = torch.max(test_outputs, 1)\n",
    "        self.plot_confusion_matrix(self.y_test.numpy(), test_preds.numpy())\n",
    "        \n",
    "        return performance_history, final_model, (final_accuracy, final_precision, final_recall, final_f1, final_auc_roc)\n",
    "\n",
    "# Usage\n",
    "labeled_data_file = 'sbert_data_with_distance_features.csv'\n",
    "augmented_data_file = 'cleaned_proscience_da.csv'\n",
    "unlabeled_data_file = r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet'\n",
    "\n",
    "pipeline = AdvancedActiveLearningPipeline(\n",
    "    labeled_data_file=labeled_data_file,\n",
    "    augmented_data_file=augmented_data_file,\n",
    "    unlabeled_data_file=unlabeled_data_file,\n",
    "    embedding_column=\"dense_embedding\",\n",
    "    label_column=\"cleaned_classification\"\n",
    ")\n",
    "\n",
    "performance_history, final_model, final_metrics = pipeline.run_pipeline()\n",
    "\n",
    "print(f\"Final Accuracy: {final_metrics[0]:.4f}\")\n",
    "print(f\"Final Precision: {final_metrics[1]:.4f}\")\n",
    "print(f\"Final Recall: {final_metrics[2]:.4f}\")\n",
    "print(f\"Final F1 Score: {final_metrics[3]:.4f}\")\n",
    "\n",
    "print(\"\\nAccuracy history saved as 'qbc_accuracy_history.png'\")\n",
    "print(\"AUC-ROC history saved as 'qbc_auc_roc_history.png'\")\n",
    "print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOFT-LABELLING: with Siamese Network and transfomer architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:49:34,600 - INFO - Initializing PyTorchActiveLearningPipeline\n",
      "2024-09-25 17:49:34,601 - INFO - Labeled data file: sbert_data_with_distance_features.csv\n",
      "2024-09-25 17:49:34,602 - INFO - Augmented data file: cleaned_proscience_da.csv\n",
      "2024-09-25 17:49:34,603 - INFO - Unlabeled data file: C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet\n",
      "2024-09-25 17:49:34,603 - INFO - Embedding column: dense_embedding\n",
      "2024-09-25 17:49:34,604 - INFO - Label column: cleaned_classification\n",
      "2024-09-25 17:49:34,605 - INFO - Test size: 0.2\n",
      "2024-09-25 17:49:34,605 - INFO - Initial labeled ratio: 0.3\n",
      "2024-09-25 17:49:34,606 - INFO - Random state: 42\n",
      "2024-09-25 17:49:34,606 - INFO - Batch size: 64\n",
      "2024-09-25 17:49:34,607 - INFO - Loading and preprocessing data...\n",
      "2024-09-25 17:49:47,018 - INFO - Labeled data shape: (79763, 13)\n",
      "2024-09-25 17:49:48,036 - INFO - Augmented data shape: (5378, 4)\n",
      "2024-09-25 17:49:50,473 - INFO - Unlabeled data shape: (412879, 384)\n",
      "2024-09-25 17:49:50,941 - INFO - FAISS index size: 412879\n",
      "2024-09-25 17:49:50,942 - INFO - Data loaded and preprocessed in 16.33 seconds\n",
      "2024-09-25 17:49:50,948 - INFO - Starting PyTorch active learning pipeline\n",
      "2024-09-25 17:49:50,948 - INFO - Preparing data for active learning\n",
      "2024-09-25 17:49:51,607 - INFO - Initial labeled set shape: (25542, 384)\n",
      "2024-09-25 17:49:51,608 - INFO - Pool set shape: (47679, 384)\n",
      "2024-09-25 17:49:51,609 - INFO - Test set shape: (11920, 384)\n",
      "2024-09-25 17:49:51,729 - INFO - Starting active learning with Siamese network and Transformer architecture\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93257dcffc7147c89158d579e4edae9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Active Learning Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:50:11,397 - INFO - Query Round 1/50\n",
      "2024-09-25 17:50:15,637 - INFO - Siamese Epoch [1/5], Loss: 44.3206\n",
      "2024-09-25 17:50:15,690 - INFO - Siamese Epoch [2/5], Loss: 7.1722\n",
      "2024-09-25 17:50:15,740 - INFO - Siamese Epoch [3/5], Loss: 4.3237\n",
      "2024-09-25 17:50:15,789 - INFO - Siamese Epoch [4/5], Loss: 4.0270\n",
      "2024-09-25 17:50:15,839 - INFO - Siamese Epoch [5/5], Loss: 4.2342\n",
      "2024-09-25 17:50:25,007 - INFO - Classifier Epoch [1/5], Loss: 0.6931\n",
      "2024-09-25 17:50:33,234 - INFO - Classifier Epoch [2/5], Loss: 0.6346\n",
      "2024-09-25 17:50:41,507 - INFO - Classifier Epoch [3/5], Loss: 0.6152\n",
      "2024-09-25 17:50:52,024 - INFO - Classifier Epoch [4/5], Loss: 0.6030\n",
      "2024-09-25 17:51:03,410 - INFO - Classifier Epoch [5/5], Loss: 0.5908\n",
      "2024-09-25 17:51:03,656 - INFO - Query Round 2/50\n",
      "2024-09-25 17:51:08,199 - INFO - Siamese Epoch [1/5], Loss: 33.9851\n",
      "2024-09-25 17:51:08,258 - INFO - Siamese Epoch [2/5], Loss: 24.3450\n",
      "2024-09-25 17:51:08,314 - INFO - Siamese Epoch [3/5], Loss: 17.1783\n",
      "2024-09-25 17:51:08,369 - INFO - Siamese Epoch [4/5], Loss: 12.8496\n",
      "2024-09-25 17:51:08,433 - INFO - Siamese Epoch [5/5], Loss: 9.9925\n",
      "2024-09-25 17:51:22,073 - INFO - Classifier Epoch [1/5], Loss: 0.5877\n",
      "2024-09-25 17:51:34,234 - INFO - Classifier Epoch [2/5], Loss: 0.5803\n",
      "2024-09-25 17:51:46,614 - INFO - Classifier Epoch [3/5], Loss: 0.5735\n",
      "2024-09-25 17:51:58,389 - INFO - Classifier Epoch [4/5], Loss: 0.5650\n",
      "2024-09-25 17:52:10,707 - INFO - Classifier Epoch [5/5], Loss: 0.5612\n",
      "2024-09-25 17:52:10,979 - INFO - Query Round 3/50\n",
      "2024-09-25 17:52:15,677 - INFO - Siamese Epoch [1/5], Loss: 31.2997\n",
      "2024-09-25 17:52:15,727 - INFO - Siamese Epoch [2/5], Loss: 20.3525\n",
      "2024-09-25 17:52:15,777 - INFO - Siamese Epoch [3/5], Loss: 16.3627\n",
      "2024-09-25 17:52:15,826 - INFO - Siamese Epoch [4/5], Loss: 12.2940\n",
      "2024-09-25 17:52:15,878 - INFO - Siamese Epoch [5/5], Loss: 9.3035\n",
      "2024-09-25 17:52:28,762 - INFO - Classifier Epoch [1/5], Loss: 0.5559\n",
      "2024-09-25 17:52:41,126 - INFO - Classifier Epoch [2/5], Loss: 0.5479\n",
      "2024-09-25 17:52:53,739 - INFO - Classifier Epoch [3/5], Loss: 0.5463\n",
      "2024-09-25 17:53:05,377 - INFO - Classifier Epoch [4/5], Loss: 0.5414\n",
      "2024-09-25 17:53:17,147 - INFO - Classifier Epoch [5/5], Loss: 0.5299\n",
      "2024-09-25 17:53:17,389 - INFO - Query Round 4/50\n",
      "2024-09-25 17:53:21,929 - INFO - Siamese Epoch [1/5], Loss: 32.3187\n",
      "2024-09-25 17:53:21,981 - INFO - Siamese Epoch [2/5], Loss: 23.3826\n",
      "2024-09-25 17:53:22,038 - INFO - Siamese Epoch [3/5], Loss: 17.9270\n",
      "2024-09-25 17:53:22,098 - INFO - Siamese Epoch [4/5], Loss: 12.6830\n",
      "2024-09-25 17:53:22,160 - INFO - Siamese Epoch [5/5], Loss: 8.6597\n",
      "2024-09-25 17:53:34,709 - INFO - Classifier Epoch [1/5], Loss: 0.5333\n",
      "2024-09-25 17:53:46,174 - INFO - Classifier Epoch [2/5], Loss: 0.5229\n",
      "2024-09-25 17:53:57,872 - INFO - Classifier Epoch [3/5], Loss: 0.5192\n",
      "2024-09-25 17:54:09,793 - INFO - Classifier Epoch [4/5], Loss: 0.5158\n",
      "2024-09-25 17:54:22,935 - INFO - Classifier Epoch [5/5], Loss: 0.5119\n",
      "2024-09-25 17:54:23,175 - INFO - Query Round 5/50\n",
      "2024-09-25 17:54:27,991 - INFO - Siamese Epoch [1/5], Loss: 27.3144\n",
      "2024-09-25 17:54:28,062 - INFO - Siamese Epoch [2/5], Loss: 18.4004\n",
      "2024-09-25 17:54:28,144 - INFO - Siamese Epoch [3/5], Loss: 12.3447\n",
      "2024-09-25 17:54:28,222 - INFO - Siamese Epoch [4/5], Loss: 8.3369\n",
      "2024-09-25 17:54:28,307 - INFO - Siamese Epoch [5/5], Loss: 6.1073\n",
      "2024-09-25 17:54:41,650 - INFO - Classifier Epoch [1/5], Loss: 0.5103\n",
      "2024-09-25 17:54:53,721 - INFO - Classifier Epoch [2/5], Loss: 0.5030\n",
      "2024-09-25 17:55:07,045 - INFO - Classifier Epoch [3/5], Loss: 0.4967\n",
      "2024-09-25 17:55:18,307 - INFO - Classifier Epoch [4/5], Loss: 0.4921\n",
      "2024-09-25 17:55:29,744 - INFO - Classifier Epoch [5/5], Loss: 0.4866\n",
      "2024-09-25 17:55:30,010 - INFO - Iteration 5/50: Accuracy = 0.7525, F1 = 0.6152, AUC-ROC = 0.8734\n",
      "2024-09-25 17:55:30,012 - INFO - Query Round 6/50\n",
      "2024-09-25 17:55:34,585 - INFO - Siamese Epoch [1/5], Loss: 29.6201\n",
      "2024-09-25 17:55:34,640 - INFO - Siamese Epoch [2/5], Loss: 20.5114\n",
      "2024-09-25 17:55:34,697 - INFO - Siamese Epoch [3/5], Loss: 12.4405\n",
      "2024-09-25 17:55:34,760 - INFO - Siamese Epoch [4/5], Loss: 7.7176\n",
      "2024-09-25 17:55:34,824 - INFO - Siamese Epoch [5/5], Loss: 5.0680\n",
      "2024-09-25 17:55:47,040 - INFO - Classifier Epoch [1/5], Loss: 0.4885\n",
      "2024-09-25 17:55:59,099 - INFO - Classifier Epoch [2/5], Loss: 0.4817\n",
      "2024-09-25 17:56:12,380 - INFO - Classifier Epoch [3/5], Loss: 0.4751\n",
      "2024-09-25 17:56:24,732 - INFO - Classifier Epoch [4/5], Loss: 0.4685\n",
      "2024-09-25 17:56:36,493 - INFO - Classifier Epoch [5/5], Loss: 0.4643\n",
      "2024-09-25 17:56:36,743 - INFO - Query Round 7/50\n",
      "2024-09-25 17:56:42,381 - INFO - Siamese Epoch [1/5], Loss: 29.4339\n",
      "2024-09-25 17:56:42,451 - INFO - Siamese Epoch [2/5], Loss: 20.7361\n",
      "2024-09-25 17:56:42,523 - INFO - Siamese Epoch [3/5], Loss: 12.1739\n",
      "2024-09-25 17:56:42,593 - INFO - Siamese Epoch [4/5], Loss: 8.7155\n",
      "2024-09-25 17:56:42,661 - INFO - Siamese Epoch [5/5], Loss: 6.4376\n",
      "2024-09-25 17:56:56,415 - INFO - Classifier Epoch [1/5], Loss: 0.4606\n",
      "2024-09-25 17:57:09,143 - INFO - Classifier Epoch [2/5], Loss: 0.4585\n",
      "2024-09-25 17:57:22,617 - INFO - Classifier Epoch [3/5], Loss: 0.4500\n",
      "2024-09-25 17:57:36,218 - INFO - Classifier Epoch [4/5], Loss: 0.4679\n",
      "2024-09-25 17:57:48,422 - INFO - Classifier Epoch [5/5], Loss: 0.4476\n",
      "2024-09-25 17:57:48,693 - INFO - Query Round 8/50\n",
      "2024-09-25 17:57:53,393 - INFO - Siamese Epoch [1/5], Loss: 37.9917\n",
      "2024-09-25 17:57:53,448 - INFO - Siamese Epoch [2/5], Loss: 22.3822\n",
      "2024-09-25 17:57:53,506 - INFO - Siamese Epoch [3/5], Loss: 15.3068\n",
      "2024-09-25 17:57:53,566 - INFO - Siamese Epoch [4/5], Loss: 9.9362\n",
      "2024-09-25 17:57:53,629 - INFO - Siamese Epoch [5/5], Loss: 6.1339\n",
      "2024-09-25 17:58:07,825 - INFO - Classifier Epoch [1/5], Loss: 0.4458\n",
      "2024-09-25 17:58:19,743 - INFO - Classifier Epoch [2/5], Loss: 0.4349\n",
      "2024-09-25 17:58:32,788 - INFO - Classifier Epoch [3/5], Loss: 0.4322\n",
      "2024-09-25 17:58:46,475 - INFO - Classifier Epoch [4/5], Loss: 0.4232\n",
      "2024-09-25 17:59:02,139 - INFO - Classifier Epoch [5/5], Loss: 0.4201\n",
      "2024-09-25 17:59:02,560 - INFO - Query Round 9/50\n",
      "2024-09-25 17:59:09,314 - INFO - Siamese Epoch [1/5], Loss: 37.8591\n",
      "2024-09-25 17:59:09,416 - INFO - Siamese Epoch [2/5], Loss: 26.9998\n",
      "2024-09-25 17:59:09,504 - INFO - Siamese Epoch [3/5], Loss: 19.5177\n",
      "2024-09-25 17:59:09,601 - INFO - Siamese Epoch [4/5], Loss: 14.2785\n",
      "2024-09-25 17:59:09,685 - INFO - Siamese Epoch [5/5], Loss: 11.3058\n",
      "2024-09-25 17:59:27,582 - INFO - Classifier Epoch [1/5], Loss: 0.4153\n",
      "2024-09-25 17:59:43,734 - INFO - Classifier Epoch [2/5], Loss: 0.4115\n",
      "2024-09-25 17:59:58,173 - INFO - Classifier Epoch [3/5], Loss: 0.4026\n",
      "2024-09-25 18:00:12,083 - INFO - Classifier Epoch [4/5], Loss: 0.4023\n",
      "2024-09-25 18:00:25,777 - INFO - Classifier Epoch [5/5], Loss: 0.3897\n",
      "2024-09-25 18:00:26,058 - INFO - Query Round 10/50\n",
      "2024-09-25 18:00:31,274 - INFO - Siamese Epoch [1/5], Loss: 55.9658\n",
      "2024-09-25 18:00:31,346 - INFO - Siamese Epoch [2/5], Loss: 37.7660\n",
      "2024-09-25 18:00:31,422 - INFO - Siamese Epoch [3/5], Loss: 23.0380\n",
      "2024-09-25 18:00:31,500 - INFO - Siamese Epoch [4/5], Loss: 14.8909\n",
      "2024-09-25 18:00:31,595 - INFO - Siamese Epoch [5/5], Loss: 9.8485\n",
      "2024-09-25 18:00:46,923 - INFO - Classifier Epoch [1/5], Loss: 0.3968\n",
      "2024-09-25 18:01:00,693 - INFO - Classifier Epoch [2/5], Loss: 0.3837\n",
      "2024-09-25 18:01:14,350 - INFO - Classifier Epoch [3/5], Loss: 0.3806\n",
      "2024-09-25 18:01:27,415 - INFO - Classifier Epoch [4/5], Loss: 0.3732\n",
      "2024-09-25 18:01:41,647 - INFO - Classifier Epoch [5/5], Loss: 0.3694\n",
      "2024-09-25 18:01:41,938 - INFO - Iteration 10/50: Accuracy = 0.7451, F1 = 0.6476, AUC-ROC = 0.8673\n",
      "2024-09-25 18:01:41,941 - INFO - Query Round 11/50\n",
      "2024-09-25 18:01:46,820 - INFO - Siamese Epoch [1/5], Loss: 45.6939\n",
      "2024-09-25 18:01:46,886 - INFO - Siamese Epoch [2/5], Loss: 33.9722\n",
      "2024-09-25 18:01:46,955 - INFO - Siamese Epoch [3/5], Loss: 19.6810\n",
      "2024-09-25 18:01:47,027 - INFO - Siamese Epoch [4/5], Loss: 12.6559\n",
      "2024-09-25 18:01:47,102 - INFO - Siamese Epoch [5/5], Loss: 8.6057\n",
      "2024-09-25 18:02:02,152 - INFO - Classifier Epoch [1/5], Loss: 0.3687\n",
      "2024-09-25 18:02:15,451 - INFO - Classifier Epoch [2/5], Loss: 0.3588\n",
      "2024-09-25 18:02:28,653 - INFO - Classifier Epoch [3/5], Loss: 0.3522\n",
      "2024-09-25 18:02:41,309 - INFO - Classifier Epoch [4/5], Loss: 0.3480\n",
      "2024-09-25 18:02:54,268 - INFO - Classifier Epoch [5/5], Loss: 0.3442\n",
      "2024-09-25 18:02:54,579 - INFO - Query Round 12/50\n",
      "2024-09-25 18:02:59,411 - INFO - Siamese Epoch [1/5], Loss: 52.5840\n",
      "2024-09-25 18:02:59,460 - INFO - Siamese Epoch [2/5], Loss: 40.3221\n",
      "2024-09-25 18:02:59,510 - INFO - Siamese Epoch [3/5], Loss: 23.5771\n",
      "2024-09-25 18:02:59,562 - INFO - Siamese Epoch [4/5], Loss: 17.4629\n",
      "2024-09-25 18:02:59,620 - INFO - Siamese Epoch [5/5], Loss: 11.2938\n",
      "2024-09-25 18:03:13,771 - INFO - Classifier Epoch [1/5], Loss: 0.3434\n",
      "2024-09-25 18:03:27,517 - INFO - Classifier Epoch [2/5], Loss: 0.3349\n",
      "2024-09-25 18:03:40,505 - INFO - Classifier Epoch [3/5], Loss: 0.3281\n",
      "2024-09-25 18:03:53,957 - INFO - Classifier Epoch [4/5], Loss: 0.3243\n",
      "2024-09-25 18:04:06,616 - INFO - Classifier Epoch [5/5], Loss: 0.3188\n",
      "2024-09-25 18:04:06,878 - INFO - Query Round 13/50\n",
      "2024-09-25 18:04:11,415 - INFO - Siamese Epoch [1/5], Loss: 41.1587\n",
      "2024-09-25 18:04:11,465 - INFO - Siamese Epoch [2/5], Loss: 31.7734\n",
      "2024-09-25 18:04:11,517 - INFO - Siamese Epoch [3/5], Loss: 20.5340\n",
      "2024-09-25 18:04:11,566 - INFO - Siamese Epoch [4/5], Loss: 14.6675\n",
      "2024-09-25 18:04:11,619 - INFO - Siamese Epoch [5/5], Loss: 9.8762\n",
      "2024-09-25 18:04:24,830 - INFO - Classifier Epoch [1/5], Loss: 0.3219\n",
      "2024-09-25 18:04:37,597 - INFO - Classifier Epoch [2/5], Loss: 0.3114\n",
      "2024-09-25 18:04:50,429 - INFO - Classifier Epoch [3/5], Loss: 0.3052\n",
      "2024-09-25 18:05:03,041 - INFO - Classifier Epoch [4/5], Loss: 0.3053\n",
      "2024-09-25 18:05:15,657 - INFO - Classifier Epoch [5/5], Loss: 0.2934\n",
      "2024-09-25 18:05:15,914 - INFO - Query Round 14/50\n",
      "2024-09-25 18:05:20,430 - INFO - Siamese Epoch [1/5], Loss: 66.6649\n",
      "2024-09-25 18:05:20,478 - INFO - Siamese Epoch [2/5], Loss: 51.1694\n",
      "2024-09-25 18:05:20,532 - INFO - Siamese Epoch [3/5], Loss: 28.4325\n",
      "2024-09-25 18:05:20,586 - INFO - Siamese Epoch [4/5], Loss: 18.6325\n",
      "2024-09-25 18:05:20,645 - INFO - Siamese Epoch [5/5], Loss: 12.5259\n",
      "2024-09-25 18:05:34,026 - INFO - Classifier Epoch [1/5], Loss: 0.3023\n",
      "2024-09-25 18:05:46,698 - INFO - Classifier Epoch [2/5], Loss: 0.2869\n",
      "2024-09-25 18:05:59,201 - INFO - Classifier Epoch [3/5], Loss: 0.2830\n",
      "2024-09-25 18:06:11,895 - INFO - Classifier Epoch [4/5], Loss: 0.2769\n",
      "2024-09-25 18:06:25,708 - INFO - Classifier Epoch [5/5], Loss: 0.2715\n",
      "2024-09-25 18:06:25,980 - INFO - Query Round 15/50\n",
      "2024-09-25 18:06:30,939 - INFO - Siamese Epoch [1/5], Loss: 52.3138\n",
      "2024-09-25 18:06:31,003 - INFO - Siamese Epoch [2/5], Loss: 35.6141\n",
      "2024-09-25 18:06:31,071 - INFO - Siamese Epoch [3/5], Loss: 21.0089\n",
      "2024-09-25 18:06:31,139 - INFO - Siamese Epoch [4/5], Loss: 11.8358\n",
      "2024-09-25 18:06:31,208 - INFO - Siamese Epoch [5/5], Loss: 9.8510\n",
      "2024-09-25 18:06:46,141 - INFO - Classifier Epoch [1/5], Loss: 0.2779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 468\u001b[0m\n\u001b[0;32m    458\u001b[0m unlabeled_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnrosso\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mthesis_project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124membeddings.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    460\u001b[0m active_learning_pipeline \u001b[38;5;241m=\u001b[39m PyTorchActiveLearningPipeline(\n\u001b[0;32m    461\u001b[0m     labeled_data_file\u001b[38;5;241m=\u001b[39mlabeled_data_file,\n\u001b[0;32m    462\u001b[0m     augmented_data_file\u001b[38;5;241m=\u001b[39maugmented_data_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m     label_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m )\n\u001b[1;32m--> 468\u001b[0m performance_history, cm, class_report, final_model \u001b[38;5;241m=\u001b[39m \u001b[43mactive_learning_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_report)\n",
      "Cell \u001b[1;32mIn[1], line 354\u001b[0m, in \u001b[0;36mPyTorchActiveLearningPipeline.run_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m--> 354\u001b[0m performance_history, final_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_active_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# Evaluate final model\u001b[39;00m\n\u001b[0;32m    357\u001b[0m final_model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[1], line 317\u001b[0m, in \u001b[0;36mPyTorchActiveLearningPipeline.train_with_active_learning\u001b[1;34m(self, n_queries, n_instances_per_query, k_similar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m labeled_loader:\n\u001b[0;32m    316\u001b[0m     optimizer_classifier\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 317\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     supervised_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, y_batch)\n\u001b[0;32m    319\u001b[0m     supervised_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 451\u001b[0m, in \u001b[0;36mSiameseClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 451\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(embeddings)\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 427\u001b[0m, in \u001b[0;36mTransformerEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    425\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layer(x)  \u001b[38;5;66;03m# [batch_size, embedding_dim]\u001b[39;00m\n\u001b[0;32m    426\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, seq_len=1, embedding_dim]\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len=1, embedding_dim]\u001b[39;00m\n\u001b[0;32m    428\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, embedding_dim]\u001b[39;00m\n\u001b[0;32m    429\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:750\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[1;32m--> 750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\torch\\nn\\functional.py:2573\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2571\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2572\u001b[0m     )\n\u001b[1;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PyTorchActiveLearningPipeline:\n",
    "    def __init__(self, labeled_data_file: str, augmented_data_file: str, unlabeled_data_file: str, \n",
    "                 embedding_column: str, label_column: str, test_size: float = 0.2, \n",
    "                 initial_labeled_ratio: float = 0.3, random_state: int = 42, batch_size: int = 64):\n",
    "        self.labeled_data_file = labeled_data_file\n",
    "        self.augmented_data_file = augmented_data_file\n",
    "        self.unlabeled_data_file = unlabeled_data_file\n",
    "        self.embedding_column = embedding_column\n",
    "        self.label_column = label_column\n",
    "        self.test_size = test_size\n",
    "        self.initial_labeled_ratio = initial_labeled_ratio\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.le = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self._log_init_info()\n",
    "        self._load_and_preprocess_data()\n",
    "        \n",
    "    def _log_init_info(self) -> None:\n",
    "        logger.info(\"Initializing PyTorchActiveLearningPipeline\")\n",
    "        logger.info(f\"Labeled data file: {self.labeled_data_file}\")\n",
    "        logger.info(f\"Augmented data file: {self.augmented_data_file}\")\n",
    "        logger.info(f\"Unlabeled data file: {self.unlabeled_data_file}\")\n",
    "        logger.info(f\"Embedding column: {self.embedding_column}\")\n",
    "        logger.info(f\"Label column: {self.label_column}\")\n",
    "        logger.info(f\"Test size: {self.test_size}\")\n",
    "        logger.info(f\"Initial labeled ratio: {self.initial_labeled_ratio}\")\n",
    "        logger.info(f\"Random state: {self.random_state}\")\n",
    "        logger.info(f\"Batch size: {self.batch_size}\")\n",
    "\n",
    "    def _process_embeddings(self, df):\n",
    "        def process_embedding(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip('[]')\n",
    "                try:\n",
    "                    return np.array([float(i) for i in x.split(',')])\n",
    "                except ValueError:\n",
    "                    return np.array([float(i) for i in x.split()])\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                return x\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected embedding format: {type(x)}\")\n",
    "\n",
    "        df[self.embedding_column] = df[self.embedding_column].apply(process_embedding)\n",
    "        return df\n",
    "\n",
    "    def _load_and_preprocess_data(self) -> None:\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        self.labeled_data = pd.read_csv(self.labeled_data_file)\n",
    "        self.labeled_data = self._process_embeddings(self.labeled_data)\n",
    "        logger.info(f\"Labeled data shape: {self.labeled_data.shape}\")\n",
    "        \n",
    "        self.augmented_data = pd.read_csv(self.augmented_data_file)\n",
    "        self.augmented_data = self._process_embeddings(self.augmented_data)\n",
    "        logger.info(f\"Augmented data shape: {self.augmented_data.shape}\")\n",
    "        \n",
    "        all_data = pd.concat([self.labeled_data, self.augmented_data], ignore_index=True)\n",
    "        \n",
    "        self.le.fit(all_data[self.label_column])\n",
    "        self.labeled_data['encoded_label'] = self.le.transform(self.labeled_data[self.label_column])\n",
    "        self.augmented_data['encoded_label'] = self.le.transform(self.augmented_data[self.label_column])\n",
    "        \n",
    "        # Load unlabeled data\n",
    "        self.dimension = len(self.labeled_data[self.embedding_column].iloc[0])\n",
    "        self.unlabeled_data = []\n",
    "        parquet_file = pq.ParquetFile(self.unlabeled_data_file)\n",
    "        \n",
    "        # Read in batches to manage memory\n",
    "        for batch in parquet_file.iter_batches(batch_size=10000, columns=[self.embedding_column]):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            df_chunk = self._process_embeddings(df_chunk)\n",
    "            embeddings = np.stack(df_chunk[self.embedding_column].values)\n",
    "            self.unlabeled_data.append(embeddings)\n",
    "        \n",
    "        self.unlabeled_data = np.vstack(self.unlabeled_data)\n",
    "        logger.info(f\"Unlabeled data shape: {self.unlabeled_data.shape}\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(self.unlabeled_data.astype('float32'))\n",
    "        logger.info(f\"FAISS index size: {self.index.ntotal}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Data loaded and preprocessed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        logger.info(\"Preparing data for active learning\")\n",
    "        \n",
    "        X = np.vstack([\n",
    "            np.stack(self.labeled_data[self.embedding_column].values),\n",
    "            np.stack(self.augmented_data[self.embedding_column].values)\n",
    "        ])\n",
    "        y = np.concatenate([\n",
    "            self.labeled_data['encoded_label'].values,\n",
    "            self.augmented_data['encoded_label'].values\n",
    "        ])\n",
    "        \n",
    "        # Ensure that the initial labeled set contains samples from all classes\n",
    "        classes = np.unique(y)\n",
    "        initial_indices = []\n",
    "        for cls in classes:\n",
    "            idx = np.where(y == cls)[0]\n",
    "            initial_indices.append(idx[0])  # Take the first instance of each class\n",
    "\n",
    "        remaining_indices = [i for i in range(len(y)) if i not in initial_indices]\n",
    "\n",
    "        X_initial = X[initial_indices]\n",
    "        y_initial = y[initial_indices]\n",
    "\n",
    "        # Now sample the rest of the initial data\n",
    "        additional_initial_size = int(self.initial_labeled_ratio * len(X)) - len(initial_indices)\n",
    "        X_remaining = X[remaining_indices]\n",
    "        y_remaining = y[remaining_indices]\n",
    "\n",
    "        X_additional_initial, X_pool, y_additional_initial, y_pool = train_test_split(\n",
    "            X_remaining, y_remaining, train_size=additional_initial_size,\n",
    "            random_state=self.random_state, stratify=y_remaining\n",
    "        )\n",
    "\n",
    "        X_initial = np.vstack([X_initial, X_additional_initial])\n",
    "        y_initial = np.concatenate([y_initial, y_additional_initial])\n",
    "\n",
    "        # Split the pool into pool and test sets\n",
    "        X_pool, X_test, y_pool, y_test = train_test_split(\n",
    "            X_pool, y_pool, test_size=self.test_size, random_state=self.random_state, stratify=y_pool\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Initial labeled set shape: {X_initial.shape}\")\n",
    "        logger.info(f\"Pool set shape: {X_pool.shape}\")\n",
    "        logger.info(f\"Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        self.X_initial = torch.tensor(X_initial, dtype=torch.float32).to(self.device)\n",
    "        self.y_initial = torch.tensor(y_initial, dtype=torch.long).to(self.device)\n",
    "        self.X_pool = torch.tensor(X_pool, dtype=torch.float32).to(self.device)\n",
    "        self.y_pool = torch.tensor(y_pool, dtype=torch.long).to(self.device)  # For simulation\n",
    "        self.X_test = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "        self.y_test = torch.tensor(y_test, dtype=torch.long).to(self.device)\n",
    "\n",
    "    def train_and_evaluate(self, classifier, X_test, y_test):\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = classifier(X_test)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred_proba = F.softmax(outputs, dim=1)\n",
    "\n",
    "            y_test_np = y_test.cpu().numpy()\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            y_pred_proba_np = y_pred_proba.cpu().numpy()\n",
    "\n",
    "            accuracy = accuracy_score(y_test_np, y_pred_np)\n",
    "            f1 = f1_score(y_test_np, y_pred_np, average='macro')\n",
    "            auc_roc = roc_auc_score(y_test_np, y_pred_proba_np, multi_class='ovr')\n",
    "\n",
    "        return accuracy, f1, auc_roc\n",
    "\n",
    "    def get_uncertainty_scores(self, classifier, X_pool):\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = classifier(X_pool)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-5), dim=1)\n",
    "        return entropy\n",
    "\n",
    "    def query_samples(self, classifier, X_pool, n_instances):\n",
    "        uncertainties = self.get_uncertainty_scores(classifier, X_pool)\n",
    "        _, query_indices = torch.topk(uncertainties, n_instances)\n",
    "        return query_indices\n",
    "\n",
    "    def find_similar_samples(self, sample, k=5):\n",
    "        sample_np = sample.cpu().numpy().reshape(1, -1).astype('float32')\n",
    "        distances, indices = self.index.search(sample_np, k)\n",
    "        return indices[0]  # Return indices of similar samples\n",
    "\n",
    "    def train_with_active_learning(self, n_queries=50, n_instances_per_query=100, k_similar=5):\n",
    "        logger.info(\"Starting active learning with Siamese network and Transformer architecture\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        input_dim = self.X_initial.shape[1]  # Should be 384\n",
    "        embedding_dim = 128  # Adjusted as per your preference\n",
    "        num_classes = len(self.le.classes_)\n",
    "        num_epochs_siamese = 5\n",
    "        num_epochs_classifier = 5\n",
    "        margin = 1.0  # For contrastive loss\n",
    "\n",
    "        # Initialize Siamese network\n",
    "        embedding_model = TransformerEmbedding(input_dim=input_dim, embedding_dim=embedding_dim).to(self.device)\n",
    "        optimizer_siamese = torch.optim.Adam(embedding_model.parameters(), lr=1e-3)\n",
    "\n",
    "        # Initialize classifier\n",
    "        classifier = SiameseClassifier(embedding_model, num_classes).to(self.device)\n",
    "        optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "        # Initialize labeled dataset\n",
    "        X_labeled = self.X_initial.clone()\n",
    "        y_labeled = self.y_initial.clone()\n",
    "\n",
    "        X_pool = self.X_pool.clone()\n",
    "        y_pool = self.y_pool.clone()\n",
    "        X_unlabeled = torch.tensor(self.unlabeled_data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        performance_history = []\n",
    "        best_accuracy = 0\n",
    "        iterations_without_improvement = 0\n",
    "        max_iterations_without_improvement = 10\n",
    "\n",
    "        for query_round in tqdm(range(n_queries), desc=\"Active Learning Progress\"):\n",
    "            logger.info(f\"Query Round {query_round + 1}/{n_queries}\")\n",
    "\n",
    "            # Query new samples\n",
    "            query_indices = self.query_samples(classifier, X_pool, n_instances_per_query)\n",
    "            X_query = X_pool[query_indices]\n",
    "            y_query = y_pool[query_indices]  # For simulation; in practice, get labels from oracle\n",
    "\n",
    "            # Find similar samples using FAISS\n",
    "            similar_indices_list = []\n",
    "            for sample in X_query:\n",
    "                indices = self.find_similar_samples(sample, k=k_similar)\n",
    "                similar_indices_list.extend(indices)\n",
    "            similar_indices = list(set(similar_indices_list))\n",
    "            X_similar = X_unlabeled[similar_indices]\n",
    "\n",
    "            # Prepare negative samples (dissimilar samples)\n",
    "            all_unlabeled_indices = set(range(len(X_unlabeled)))\n",
    "            negative_indices = list(all_unlabeled_indices - set(similar_indices))\n",
    "            if len(negative_indices) >= len(X_query):\n",
    "                negative_sample_indices = np.random.choice(negative_indices, size=len(X_query), replace=False)\n",
    "            else:\n",
    "                negative_sample_indices = np.random.choice(negative_indices, size=len(X_query), replace=True)\n",
    "            X_negative = X_unlabeled[negative_sample_indices]\n",
    "\n",
    "            # Train the Siamese network\n",
    "            for epoch in range(num_epochs_siamese):\n",
    "                embedding_model.train()\n",
    "                optimizer_siamese.zero_grad()\n",
    "\n",
    "                # Get embeddings\n",
    "                embedding_anchor = embedding_model(X_query)\n",
    "                embedding_positive = embedding_model(X_similar[:len(X_query)])\n",
    "                embedding_negative = embedding_model(X_negative)\n",
    "\n",
    "                # Labels\n",
    "                labels_positive = torch.ones(len(embedding_anchor)).to(self.device)\n",
    "                labels_negative = torch.zeros(len(embedding_anchor)).to(self.device)\n",
    "\n",
    "                # Contrastive loss\n",
    "                contrastive_loss_fn = ContrastiveLoss(margin=margin)\n",
    "                loss_positive = contrastive_loss_fn(embedding_anchor, embedding_positive, labels_positive)\n",
    "                loss_negative = contrastive_loss_fn(embedding_anchor, embedding_negative, labels_negative)\n",
    "\n",
    "                loss = loss_positive + loss_negative\n",
    "                loss.backward()\n",
    "                optimizer_siamese.step()\n",
    "\n",
    "                logger.info(f\"Siamese Epoch [{epoch+1}/{num_epochs_siamese}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # Update labeled dataset\n",
    "            X_labeled = torch.cat([X_labeled, X_query], dim=0)\n",
    "            y_labeled = torch.cat([y_labeled, y_query], dim=0)\n",
    "\n",
    "            # Remove queried samples from X_pool and y_pool\n",
    "            mask_pool = torch.ones(len(X_pool), dtype=torch.bool).to(self.device)\n",
    "            mask_pool[query_indices] = False\n",
    "            X_pool = X_pool[mask_pool]\n",
    "            y_pool = y_pool[mask_pool]\n",
    "\n",
    "            # Remove similar and negative samples from X_unlabeled\n",
    "            used_indices = similar_indices + negative_sample_indices.tolist()\n",
    "            mask_unlabeled = torch.ones(len(X_unlabeled), dtype=torch.bool).to(self.device)\n",
    "            mask_unlabeled[used_indices] = False\n",
    "            X_unlabeled = X_unlabeled[mask_unlabeled]\n",
    "\n",
    "            # Rebuild FAISS index\n",
    "            if len(X_unlabeled) > 0:\n",
    "                self.index = faiss.IndexFlatL2(self.dimension)\n",
    "                self.index.add(X_unlabeled.cpu().numpy().astype('float32'))\n",
    "            else:\n",
    "                logger.info(\"No more unlabeled data to build FAISS index.\")\n",
    "                break\n",
    "\n",
    "            # Fine-tune classifier\n",
    "            labeled_dataset = LabeledDataset(X_labeled, y_labeled)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            for epoch in range(num_epochs_classifier):\n",
    "                classifier.train()\n",
    "                total_loss = 0.0\n",
    "                for X_batch, y_batch in labeled_loader:\n",
    "                    optimizer_classifier.zero_grad()\n",
    "                    outputs = classifier(X_batch)\n",
    "                    supervised_loss = F.cross_entropy(outputs, y_batch)\n",
    "                    supervised_loss.backward()\n",
    "                    optimizer_classifier.step()\n",
    "                    total_loss += supervised_loss.item()\n",
    "                logger.info(f\"Classifier Epoch [{epoch+1}/{num_epochs_classifier}], Loss: {total_loss/len(labeled_loader):.4f}\")\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy, f1, auc_roc = self.train_and_evaluate(classifier, self.X_test, self.y_test)\n",
    "            performance_history.append((accuracy, f1, auc_roc))\n",
    "\n",
    "            # Early stopping logic\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                iterations_without_improvement = 0\n",
    "                best_model = classifier\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "\n",
    "            if (query_round + 1) % 5 == 0:\n",
    "                logger.info(f\"Iteration {query_round + 1}/{n_queries}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "            if iterations_without_improvement >= max_iterations_without_improvement:\n",
    "                logger.info(f\"Early stopping at iteration {query_round + 1} due to no improvement\")\n",
    "                break\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Active learning completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Final performance: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "        return performance_history, best_model\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        logger.info(\"Starting PyTorch active learning pipeline\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.prepare_data()\n",
    "        performance_history, final_model = self.train_with_active_learning()\n",
    "\n",
    "        # Evaluate final model\n",
    "        final_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = final_model(self.X_test)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            y_test_np = self.y_test.cpu().numpy()\n",
    "\n",
    "            cm = confusion_matrix(y_test_np, y_pred_np)\n",
    "            class_report = classification_report(y_test_np, y_pred_np, target_names=self.le.classes_)\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"PyTorch Active Learning Pipeline completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(\"Classification Report:\\n\" + class_report)\n",
    "\n",
    "        self.plot_results(performance_history)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "\n",
    "        return performance_history, cm, class_report, final_model\n",
    "\n",
    "    def plot_results(self, performance_history):\n",
    "        accuracies, f1_scores, auc_rocs = zip(*performance_history)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(accuracies, label='Accuracy')\n",
    "        plt.plot(f1_scores, label='F1 Score')\n",
    "        plt.plot(auc_rocs, label='AUC-ROC')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Model Performance over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pytorch_active_learning_results.png')\n",
    "        logger.info(\"Results plot saved as 'pytorch_active_learning_results.png'\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pytorch_confusion_matrix.png')\n",
    "        logger.info(\"Confusion matrix saved as 'pytorch_confusion_matrix.png'\")\n",
    "        plt.close()\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_heads=4, num_layers=2):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.embedding_layer = nn.Linear(input_dim, embedding_dim)\n",
    "        # Set batch_first=True to resolve the warning\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        x = self.embedding_layer(x)  # [batch_size, embedding_dim]\n",
    "        x = x.unsqueeze(1)  # [batch_size, seq_len=1, embedding_dim]\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_len=1, embedding_dim]\n",
    "        x = x.squeeze(1)  # [batch_size, embedding_dim]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        # label: 1 if similar, 0 if dissimilar\n",
    "        euclidean_distance = F.pairwise_distance(embedding1, embedding2)\n",
    "        loss = (label) * torch.pow(euclidean_distance, 2) + \\\n",
    "               (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class SiameseClassifier(nn.Module):\n",
    "    def __init__(self, embedding_model, num_classes):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.fc = nn.Linear(embedding_model.output_layer.out_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding_model(x)\n",
    "        logits = self.fc(embeddings)\n",
    "        return logits\n",
    "\n",
    "# Usage\n",
    "labeled_data_file = 'sbert_data_with_distance_features.csv'\n",
    "augmented_data_file = 'cleaned_proscience_da.csv'\n",
    "unlabeled_data_file = r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet'\n",
    "\n",
    "active_learning_pipeline = PyTorchActiveLearningPipeline(\n",
    "    labeled_data_file=labeled_data_file,\n",
    "    augmented_data_file=augmented_data_file,\n",
    "    unlabeled_data_file=unlabeled_data_file,\n",
    "    embedding_column=\"dense_embedding\",\n",
    "    label_column=\"cleaned_classification\"\n",
    ")\n",
    "\n",
    "performance_history, cm, class_report, final_model = active_learning_pipeline.run_pipeline()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"Accuracy: {performance_history[-1][0]:.4f}\")\n",
    "print(f\"F1 Score: {performance_history[-1][1]:.4f}\")\n",
    "print(f\"AUC-ROC: {performance_history[-1][2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 18:38:25,801 - INFO - Initializing PyTorchActiveLearningPipeline\n",
      "2024-09-25 18:38:25,802 - INFO - Labeled data file: sbert_data_with_distance_features.csv\n",
      "2024-09-25 18:38:25,802 - INFO - Augmented data file: cleaned_proscience_da.csv\n",
      "2024-09-25 18:38:25,803 - INFO - Unlabeled data file: C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet\n",
      "2024-09-25 18:38:25,803 - INFO - Embedding column: dense_embedding\n",
      "2024-09-25 18:38:25,804 - INFO - Label column: cleaned_classification\n",
      "2024-09-25 18:38:25,804 - INFO - Test size: 0.2\n",
      "2024-09-25 18:38:25,805 - INFO - Initial labeled ratio: 0.3\n",
      "2024-09-25 18:38:25,806 - INFO - Random state: 42\n",
      "2024-09-25 18:38:25,807 - INFO - Batch size: 64\n",
      "2024-09-25 18:38:25,808 - INFO - Loading and preprocessing data...\n",
      "2024-09-25 18:38:38,054 - INFO - Labeled data shape: (79763, 13)\n",
      "2024-09-25 18:38:39,048 - INFO - Augmented data shape: (5378, 4)\n",
      "2024-09-25 18:38:41,543 - INFO - Unlabeled data shape: (412879, 384)\n",
      "2024-09-25 18:38:42,045 - INFO - FAISS index size: 412879\n",
      "2024-09-25 18:38:42,046 - INFO - Data loaded and preprocessed in 16.24 seconds\n",
      "2024-09-25 18:38:42,052 - INFO - Starting PyTorch active learning pipeline\n",
      "2024-09-25 18:38:42,053 - INFO - Preparing data for active learning\n",
      "2024-09-25 18:38:42,713 - INFO - Initial labeled set shape: (25542, 384)\n",
      "2024-09-25 18:38:42,714 - INFO - Pool set shape: (47679, 384)\n",
      "2024-09-25 18:38:42,714 - INFO - Test set shape: (11920, 384)\n",
      "2024-09-25 18:38:42,851 - INFO - Starting active learning with Uncertainty Sampling, FAISS, and Consistency Regularization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0023ba7178343fbb22e4b8f577d47db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Active Learning Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 18:39:00,867 - INFO - Query Round 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6008\\2508435316.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0membedding_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dense_embedding\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mlabel_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cleaned_classification\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m \u001b[0mperformance_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactive_learning_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Classification Report:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6008\\2508435316.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting PyTorch active learning pipeline\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[0mperformance_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_with_active_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;31m# Evaluate final model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6008\\2508435316.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, n_queries, n_instances_per_query, k_similar)\u001b[0m\n\u001b[0;32m    255\u001b[0m                     \u001b[1;31m# Consistency Regularization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                     \u001b[1;31m# Sample a batch of unlabeled data similar to X_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m                         \u001b[1;31m# For each sample in X_batch, find similar embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                         \u001b[0msimilar_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_similar_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk_similar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m                         \u001b[0msimilar_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlabeled_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msimilar_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                         \u001b[1;31m# Convert to tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                         \u001b[0mX_unlabeled_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilar_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6008\\2508435316.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, embeddings, k)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_similar_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0membeddings_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mindices\u001b[0m  \u001b[1;31m# Returns indices of similar embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\faiss\\class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nrosso\\Documents\\thesis_project\\thesis_env\\Lib\\site-packages\\faiss\\swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, n, x, k, distances, labels, params)\u001b[0m\n\u001b[0;32m   2260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexFlat_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PyTorchActiveLearningPipeline:\n",
    "    def __init__(self, labeled_data_file: str, augmented_data_file: str, unlabeled_data_file: str, \n",
    "                 embedding_column: str, label_column: str, test_size: float = 0.2, \n",
    "                 initial_labeled_ratio: float = 0.3, random_state: int = 42, batch_size: int = 64):\n",
    "        self.labeled_data_file = labeled_data_file\n",
    "        self.augmented_data_file = augmented_data_file\n",
    "        self.unlabeled_data_file = unlabeled_data_file\n",
    "        self.embedding_column = embedding_column\n",
    "        self.label_column = label_column\n",
    "        self.test_size = test_size\n",
    "        self.initial_labeled_ratio = initial_labeled_ratio\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.le = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self._log_init_info()\n",
    "        self._load_and_preprocess_data()\n",
    "        \n",
    "    def _log_init_info(self) -> None:\n",
    "        logger.info(\"Initializing PyTorchActiveLearningPipeline\")\n",
    "        logger.info(f\"Labeled data file: {self.labeled_data_file}\")\n",
    "        logger.info(f\"Augmented data file: {self.augmented_data_file}\")\n",
    "        logger.info(f\"Unlabeled data file: {self.unlabeled_data_file}\")\n",
    "        logger.info(f\"Embedding column: {self.embedding_column}\")\n",
    "        logger.info(f\"Label column: {self.label_column}\")\n",
    "        logger.info(f\"Test size: {self.test_size}\")\n",
    "        logger.info(f\"Initial labeled ratio: {self.initial_labeled_ratio}\")\n",
    "        logger.info(f\"Random state: {self.random_state}\")\n",
    "        logger.info(f\"Batch size: {self.batch_size}\")\n",
    "\n",
    "    def _process_embeddings(self, df):\n",
    "        def process_embedding(x):\n",
    "            if isinstance(x, str):\n",
    "                x = x.strip('[]')\n",
    "                try:\n",
    "                    return np.array([float(i) for i in x.split(',')])\n",
    "                except ValueError:\n",
    "                    return np.array([float(i) for i in x.split()])\n",
    "            elif isinstance(x, np.ndarray):\n",
    "                return x\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected embedding format: {type(x)}\")\n",
    "\n",
    "        df[self.embedding_column] = df[self.embedding_column].apply(process_embedding)\n",
    "        return df\n",
    "\n",
    "    def _load_and_preprocess_data(self) -> None:\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        # Load labeled data\n",
    "        self.labeled_data = pd.read_csv(self.labeled_data_file)\n",
    "        self.labeled_data = self._process_embeddings(self.labeled_data)\n",
    "        logger.info(f\"Labeled data shape: {self.labeled_data.shape}\")\n",
    "        \n",
    "        # Load augmented data\n",
    "        self.augmented_data = pd.read_csv(self.augmented_data_file)\n",
    "        self.augmented_data = self._process_embeddings(self.augmented_data)\n",
    "        logger.info(f\"Augmented data shape: {self.augmented_data.shape}\")\n",
    "        \n",
    "        all_data = pd.concat([self.labeled_data, self.augmented_data], ignore_index=True)\n",
    "        \n",
    "        self.le.fit(all_data[self.label_column])\n",
    "        self.labeled_data['encoded_label'] = self.le.transform(self.labeled_data[self.label_column])\n",
    "        self.augmented_data['encoded_label'] = self.le.transform(self.augmented_data[self.label_column])\n",
    "        \n",
    "        # Load unlabeled data\n",
    "        self.dimension = len(self.labeled_data[self.embedding_column].iloc[0])\n",
    "        self.unlabeled_data = []\n",
    "        parquet_file = pq.ParquetFile(self.unlabeled_data_file)\n",
    "        \n",
    "        # Read in batches to manage memory\n",
    "        for batch in parquet_file.iter_batches(batch_size=10000, columns=[self.embedding_column]):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            df_chunk = self._process_embeddings(df_chunk)\n",
    "            embeddings = np.stack(df_chunk[self.embedding_column].values)\n",
    "            self.unlabeled_data.append(embeddings)\n",
    "        \n",
    "        self.unlabeled_data = np.vstack(self.unlabeled_data)\n",
    "        logger.info(f\"Unlabeled data shape: {self.unlabeled_data.shape}\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(self.unlabeled_data.astype('float32'))\n",
    "        logger.info(f\"FAISS index size: {self.index.ntotal}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Data loaded and preprocessed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        logger.info(\"Preparing data for active learning\")\n",
    "        \n",
    "        X = np.vstack([\n",
    "            np.stack(self.labeled_data[self.embedding_column].values),\n",
    "            np.stack(self.augmented_data[self.embedding_column].values)\n",
    "        ])\n",
    "        y = np.concatenate([\n",
    "            self.labeled_data['encoded_label'].values,\n",
    "            self.augmented_data['encoded_label'].values\n",
    "        ])\n",
    "        \n",
    "        # Ensure that the initial labeled set contains samples from all classes\n",
    "        classes = np.unique(y)\n",
    "        initial_indices = []\n",
    "        for cls in classes:\n",
    "            idx = np.where(y == cls)[0]\n",
    "            initial_indices.append(idx[0])  # Take the first instance of each class\n",
    "\n",
    "        remaining_indices = [i for i in range(len(y)) if i not in initial_indices]\n",
    "\n",
    "        X_initial = X[initial_indices]\n",
    "        y_initial = y[initial_indices]\n",
    "\n",
    "        # Now sample the rest of the initial data\n",
    "        additional_initial_size = int(self.initial_labeled_ratio * len(X)) - len(initial_indices)\n",
    "        X_remaining = X[remaining_indices]\n",
    "        y_remaining = y[remaining_indices]\n",
    "\n",
    "        X_additional_initial, X_pool, y_additional_initial, y_pool = train_test_split(\n",
    "            X_remaining, y_remaining, train_size=additional_initial_size,\n",
    "            random_state=self.random_state, stratify=y_remaining\n",
    "        )\n",
    "\n",
    "        X_initial = np.vstack([X_initial, X_additional_initial])\n",
    "        y_initial = np.concatenate([y_initial, y_additional_initial])\n",
    "\n",
    "        # Split the pool into pool and test sets\n",
    "        X_pool, X_test, y_pool, y_test = train_test_split(\n",
    "            X_pool, y_pool, test_size=self.test_size, random_state=self.random_state, stratify=y_pool\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Initial labeled set shape: {X_initial.shape}\")\n",
    "        logger.info(f\"Pool set shape: {X_pool.shape}\")\n",
    "        logger.info(f\"Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        self.X_initial = torch.tensor(X_initial, dtype=torch.float32).to(self.device)\n",
    "        self.y_initial = torch.tensor(y_initial, dtype=torch.long).to(self.device)\n",
    "        self.X_pool = torch.tensor(X_pool, dtype=torch.float32).to(self.device)\n",
    "        self.y_pool = torch.tensor(y_pool, dtype=torch.long).to(self.device)  # For simulation\n",
    "        self.X_test = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
    "        self.y_test = torch.tensor(y_test, dtype=torch.long).to(self.device)\n",
    "\n",
    "    def train_and_evaluate(self, classifier, X_test, y_test):\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = classifier(X_test)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred_proba = F.softmax(outputs, dim=1)\n",
    "\n",
    "            y_test_np = y_test.cpu().numpy()\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            y_pred_proba_np = y_pred_proba.cpu().numpy()\n",
    "\n",
    "            accuracy = accuracy_score(y_test_np, y_pred_np)\n",
    "            f1 = f1_score(y_test_np, y_pred_np, average='macro')\n",
    "            auc_roc = roc_auc_score(y_test_np, y_pred_proba_np, multi_class='ovr')\n",
    "\n",
    "        return accuracy, f1, auc_roc\n",
    "\n",
    "    def get_uncertainty_scores(self, classifier, X_pool):\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = classifier(X_pool)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-5), dim=1)\n",
    "        return entropy\n",
    "\n",
    "    def query_samples(self, classifier, X_pool, n_instances):\n",
    "        uncertainties = self.get_uncertainty_scores(classifier, X_pool)\n",
    "        _, query_indices = torch.topk(uncertainties, n_instances)\n",
    "        return query_indices\n",
    "\n",
    "    def find_similar_embeddings(self, embeddings, k=5):\n",
    "        embeddings_np = embeddings.cpu().numpy().astype('float32')\n",
    "        distances, indices = self.index.search(embeddings_np, k)\n",
    "        return indices  # Returns indices of similar embeddings\n",
    "\n",
    "    def train_with_active_learning(self, n_queries=50, n_instances_per_query=100, k_similar=5):\n",
    "        logger.info(\"Starting active learning with Uncertainty Sampling, FAISS, and Consistency Regularization\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        input_dim = self.X_initial.shape[1]  # Embedding dimension\n",
    "        num_classes = len(self.le.classes_)\n",
    "        num_epochs = 10\n",
    "        lambda_u = 0.1  # Weight for unsupervised loss\n",
    "        noise_std = 0.1  # Standard deviation for Gaussian noise\n",
    "\n",
    "        # Initialize classifier\n",
    "        classifier = SimpleFeedforwardClassifier(input_dim, num_classes).to(self.device)\n",
    "\n",
    "        # Handle class imbalance\n",
    "        class_counts = np.bincount(self.y_initial.cpu().numpy())\n",
    "        class_weights = 1.0 / class_counts\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "        # Initialize labeled dataset\n",
    "        X_labeled = self.X_initial.clone()\n",
    "        y_labeled = self.y_initial.clone()\n",
    "\n",
    "        X_pool = self.X_pool.clone()\n",
    "        y_pool = self.y_pool.clone()\n",
    "\n",
    "        performance_history = []\n",
    "        best_accuracy = 0\n",
    "        iterations_without_improvement = 0\n",
    "        max_iterations_without_improvement = 10\n",
    "\n",
    "        for query_round in tqdm(range(n_queries), desc=\"Active Learning Progress\"):\n",
    "            logger.info(f\"Query Round {query_round + 1}/{n_queries}\")\n",
    "\n",
    "            # Train classifier\n",
    "            labeled_dataset = LabeledDataset(X_labeled, y_labeled)\n",
    "            labeled_loader = DataLoader(labeled_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "            classifier.train()\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0.0\n",
    "                for X_batch, y_batch in labeled_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = classifier(X_batch)\n",
    "                    supervised_loss = criterion(outputs, y_batch)\n",
    "                    \n",
    "                    # Consistency Regularization\n",
    "                    # Sample a batch of unlabeled data similar to X_batch\n",
    "                    with torch.no_grad():\n",
    "                        # For each sample in X_batch, find similar embeddings\n",
    "                        similar_indices = self.find_similar_embeddings(X_batch, k=k_similar)\n",
    "                        similar_embeddings = self.unlabeled_data[similar_indices.flatten()]\n",
    "                        # Convert to tensor\n",
    "                        X_unlabeled_batch = torch.tensor(similar_embeddings, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    # Apply noise to embeddings\n",
    "                    noise = torch.randn_like(X_unlabeled_batch) * noise_std\n",
    "                    X_unlabeled_aug = X_unlabeled_batch + noise\n",
    "\n",
    "                    # Compute consistency loss\n",
    "                    with torch.no_grad():\n",
    "                        preds_unlabeled = classifier(X_unlabeled_batch)\n",
    "                    preds_unlabeled_aug = classifier(X_unlabeled_aug)\n",
    "                    consistency_loss = F.mse_loss(preds_unlabeled_aug, preds_unlabeled)\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = supervised_loss + lambda_u * consistency_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                # Optionally log training loss\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy, f1, auc_roc = self.train_and_evaluate(classifier, self.X_test, self.y_test)\n",
    "            performance_history.append((accuracy, f1, auc_roc))\n",
    "\n",
    "            # Early stopping logic\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                iterations_without_improvement = 0\n",
    "                best_model = classifier\n",
    "            else:\n",
    "                iterations_without_improvement += 1\n",
    "\n",
    "            if (query_round + 1) % 5 == 0:\n",
    "                logger.info(f\"Iteration {query_round + 1}/{n_queries}: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "            if iterations_without_improvement >= max_iterations_without_improvement:\n",
    "                logger.info(f\"Early stopping at iteration {query_round + 1} due to no improvement\")\n",
    "                break\n",
    "\n",
    "            # Query new samples using Uncertainty Sampling\n",
    "            n_instances = min(n_instances_per_query, len(X_pool))\n",
    "            if n_instances == 0:\n",
    "                logger.info(\"No more samples to query from the pool.\")\n",
    "                break\n",
    "\n",
    "            query_indices = self.query_samples(classifier, X_pool, n_instances)\n",
    "            X_query = X_pool[query_indices]\n",
    "            y_query = y_pool[query_indices]  # For simulation; in practice, get labels from oracle\n",
    "\n",
    "            # Update labeled dataset with queried samples\n",
    "            X_labeled = torch.cat([X_labeled, X_query], dim=0)\n",
    "            y_labeled = torch.cat([y_labeled, y_query], dim=0)\n",
    "\n",
    "            # Remove queried samples from X_pool and y_pool\n",
    "            mask_pool = torch.ones(len(X_pool), dtype=torch.bool).to(self.device)\n",
    "            mask_pool[query_indices] = False\n",
    "            X_pool = X_pool[mask_pool]\n",
    "            y_pool = y_pool[mask_pool]\n",
    "\n",
    "            # Update class weights\n",
    "            class_counts = np.bincount(y_labeled.cpu().numpy(), minlength=num_classes)\n",
    "            class_weights = 1.0 / (class_counts + 1e-6)  # Add small value to avoid division by zero\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"Active learning completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Final performance: Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, AUC-ROC = {auc_roc:.4f}\")\n",
    "\n",
    "        return performance_history, best_model\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        logger.info(\"Starting PyTorch active learning pipeline\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.prepare_data()\n",
    "        performance_history, final_model = self.train_with_active_learning()\n",
    "\n",
    "        # Evaluate final model\n",
    "        final_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = final_model(self.X_test)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            y_test_np = self.y_test.cpu().numpy()\n",
    "\n",
    "            cm = confusion_matrix(y_test_np, y_pred_np)\n",
    "            class_report = classification_report(y_test_np, y_pred_np, target_names=self.le.classes_)\n",
    "\n",
    "        end_time = time.time()\n",
    "        logger.info(f\"PyTorch Active Learning Pipeline completed in {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(\"Classification Report:\\n\" + class_report)\n",
    "\n",
    "        self.plot_results(performance_history)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "\n",
    "        return performance_history, cm, class_report, final_model\n",
    "\n",
    "    def plot_results(self, performance_history):\n",
    "        accuracies, f1_scores, auc_rocs = zip(*performance_history)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(accuracies, label='Accuracy')\n",
    "        plt.plot(f1_scores, label='F1 Score')\n",
    "        plt.plot(auc_rocs, label='AUC-ROC')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Model Performance over Active Learning Iterations')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pytorch_active_learning_results.png')\n",
    "        logger.info(\"Results plot saved as 'pytorch_active_learning_results.png'\")\n",
    "        plt.close()\n",
    "\n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.le.classes_, yticklabels=self.le.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pytorch_confusion_matrix.png')\n",
    "        logger.info(\"Confusion matrix saved as 'pytorch_confusion_matrix.png'\")\n",
    "        plt.close()\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SimpleFeedforwardClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleFeedforwardClassifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Usage\n",
    "labeled_data_file = 'sbert_data_with_distance_features.csv'\n",
    "augmented_data_file = 'cleaned_proscience_da.csv'\n",
    "unlabeled_data_file = r'C:\\Users\\nrosso\\Documents\\thesis_project\\data\\processed\\embeddings.parquet'\n",
    "\n",
    "active_learning_pipeline = PyTorchActiveLearningPipeline(\n",
    "    labeled_data_file=labeled_data_file,\n",
    "    augmented_data_file=augmented_data_file,\n",
    "    unlabeled_data_file=unlabeled_data_file,\n",
    "    embedding_column=\"dense_embedding\",\n",
    "    label_column=\"cleaned_classification\"\n",
    ")\n",
    "\n",
    "performance_history, cm, class_report, final_model = active_learning_pipeline.run_pipeline()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"Accuracy: {performance_history[-1][0]:.4f}\")\n",
    "print(f\"F1 Score: {performance_history[-1][1]:.4f}\")\n",
    "print(f\"AUC-ROC: {performance_history[-1][2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
